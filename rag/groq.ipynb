{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key: str = os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGroq(temperature=0, groq_api_key=groq_api_key, model_name=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure, I\\'d be happy to explain!\\n\\nLLM stands for \"Low Latency Logging,\" which is a method of recording and transmitting data with minimal delay. Low latency LLMs are particularly important in time-sensitive applications where real-time data processing is critical. Here are some reasons why low latency LLMs are important:\\n\\n1. Improved user experience: In applications such as online gaming or video conferencing, low latency LLMs ensure that data is transmitted and processed quickly, resulting in a smoother and more responsive user experience.\\n2. Increased efficiency: Low latency LLMs enable faster data processing, which can lead to increased efficiency in data-intensive applications such as financial trading or scientific simulations.\\n3. Enhanced security: In security-critical applications, low latency LLMs can help detect and respond to threats more quickly, reducing the risk of data breaches or other security incidents.\\n4. Better decision-making: In real-time decision-making scenarios, such as autonomous vehicles or industrial automation, low latency LLMs can provide the necessary data in near real-time, enabling faster and more accurate decision-making.\\n\\nOverall, low latency LLMs are essential in applications where timely and accurate data processing is critical. By minimizing delay and ensuring fast data transmission, low latency LLMs can improve user experience, increase efficiency, enhance security, and enable better decision-making.', response_metadata={'token_usage': {'completion_time': 0.551, 'completion_tokens': 309, 'prompt_time': 0.008, 'prompt_tokens': 26, 'queue_time': None, 'total_time': 0.559, 'total_tokens': 335}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-4de62083-490d-4429-86b3-4f63b891eac1-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"There's a bright ball of gas in the sky,\\nThat rises and sets, making spirits high.\\nIt gives us light and warmth,\\nOn sunny days it transforms,\\nThe earth into a playground, oh my!\", response_metadata={'token_usage': {'completion_time': 0.091, 'completion_tokens': 51, 'prompt_time': 0.005, 'prompt_tokens': 18, 'queue_time': None, 'total_time': 0.096, 'total_tokens': 69}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-d3fb74dc-c74e-44da-bcc0-0d4b5f761ac1-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"Write a Limerick about {topic}\")])\n",
    "chain = prompt | chat\n",
    "await chain.ainvoke({\"topic\": \"The Sun\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silvery glow bright\n",
      "Luna's gentle light shines down\n",
      "Midnight's gentle queen"
     ]
    }
   ],
   "source": [
    "chat = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\")\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"Write a haiku about {topic}\")])\n",
    "chain = prompt | chat\n",
    "for chunk in chain.stream({\"topic\": \"The Moon\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
