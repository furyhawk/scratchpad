{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841f8ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:09:39.109194Z",
     "iopub.status.busy": "2023-02-27T15:09:39.108649Z",
     "iopub.status.idle": "2023-02-27T15:14:12.380510Z",
     "shell.execute_reply": "2023-02-27T15:14:12.378993Z"
    },
    "papermill": {
     "duration": 273.288151,
     "end_time": "2023-02-27T15:14:12.382913",
     "exception": false,
     "start_time": "2023-02-27T15:09:39.094762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /root/.config/pip/pip.conf\r\n",
      "Writing to /root/.config/pip/pip.conf\r\n",
      "Processing /kaggle/input/pytorch112-cu113/torch-1.12.1+cu113-cp37-cp37m-linux_x86_64.whl\r\n",
      "Processing /kaggle/input/pytorch112-cu113/torchvision-0.13.1+cu113-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.12.1+cu113) (4.1.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1+cu113) (9.1.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1+cu113) (1.21.6)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1+cu113) (2.28.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu113) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu113) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu113) (2.1.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu113) (1.26.14)\r\n",
      "Installing collected packages: torch, torchvision\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.11.0\r\n",
      "    Uninstalling torch-1.11.0:\r\n",
      "      Successfully uninstalled torch-1.11.0\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.12.0\r\n",
      "    Uninstalling torchvision-0.12.0:\r\n",
      "      Successfully uninstalled torchvision-0.12.0\r\n",
      "Successfully installed torch-1.12.1+cu113 torchvision-0.13.1+cu113\r\n",
      "Found existing installation: timm 0.6.12\r\n",
      "Uninstalling timm-0.6.12:\r\n",
      "  Successfully uninstalled timm-0.6.12\r\n",
      "/kaggle/tmp/libs/timm\n",
      "Obtaining file:///kaggle/tmp/libs/timm\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.7/site-packages (from timm==0.8.11.dev0) (1.12.1+cu113)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm==0.8.11.dev0) (0.13.1+cu113)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm==0.8.11.dev0) (6.0)\r\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.7/site-packages (from timm==0.8.11.dev0) (0.10.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->timm==0.8.11.dev0) (4.1.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub->timm==0.8.11.dev0) (23.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from huggingface_hub->timm==0.8.11.dev0) (4.64.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface_hub->timm==0.8.11.dev0) (6.0.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface_hub->timm==0.8.11.dev0) (3.7.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface_hub->timm==0.8.11.dev0) (2.28.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.8.11.dev0) (9.1.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.8.11.dev0) (1.21.6)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface_hub->timm==0.8.11.dev0) (3.8.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub->timm==0.8.11.dev0) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub->timm==0.8.11.dev0) (2022.12.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub->timm==0.8.11.dev0) (1.26.14)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub->timm==0.8.11.dev0) (2.1.0)\r\n",
      "Installing collected packages: timm\r\n",
      "  Running setup.py develop for timm\r\n",
      "Successfully installed timm-0.8.11.dev0\r\n",
      "/kaggle/working\n",
      "Processing /kaggle/input/torch-tensorrt-pkg/nvidia_pyindex-1.0.9-py3-none-any.whl\r\n",
      "Installing collected packages: nvidia-pyindex\r\n",
      "Successfully installed nvidia-pyindex-1.0.9\r\n",
      "Looking in links: /tmp/pip/cache/\r\n",
      "Processing /tmp/pip/cache/nvidia_tensorrt-8.4.3.1-cp37-none-linux_x86_64.whl\r\n",
      "Processing /tmp/pip/cache/nvidia-cudnn-cu11-2022.5.19.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hProcessing /tmp/pip/cache/nvidia-cuda-runtime-cu11-2022.4.25.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hProcessing /tmp/pip/cache/nvidia-cublas-cu11-2022.4.8.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hProcessing /tmp/pip/cache/nvidia_cublas_cu117-11.10.1.25-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /tmp/pip/cache/nvidia_cuda_runtime_cu117-11.7.60-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /tmp/pip/cache/nvidia_cudnn_cu116-8.4.0.27-py3-none-manylinux1_x86_64.whl\r\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu117->nvidia-cublas-cu11->nvidia_tensorrt) (0.37.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu117->nvidia-cublas-cu11->nvidia_tensorrt) (59.8.0)\r\n",
      "Building wheels for collected packages: nvidia-cublas-cu11, nvidia-cuda-runtime-cu11, nvidia-cudnn-cu11\r\n",
      "  Building wheel for nvidia-cublas-cu11 (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for nvidia-cublas-cu11: filename=nvidia_cublas_cu11-2022.4.8-py3-none-any.whl size=15624 sha256=b0a626327258e41fe2755049e867d4eea7c26ab539c7141226d2bbe5de3f93ea\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/b2/6a/28/9f1c745aca384aa759d6c5d13bf0fc1ab5fe1305fa7331f8b4\r\n",
      "  Building wheel for nvidia-cuda-runtime-cu11 (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for nvidia-cuda-runtime-cu11: filename=nvidia_cuda_runtime_cu11-2022.4.25-py3-none-any.whl size=15696 sha256=75e894b4dc8ca8bd3a45c48e2ab4979c13e76c33b273dd475cb5dc03b028dd7c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/01/be/03d85773bc3ab0e592f4aca592c04c998a0268bc3d4764a47d\r\n",
      "  Building wheel for nvidia-cudnn-cu11 (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for nvidia-cudnn-cu11: filename=nvidia_cudnn_cu11-2022.5.19-py3-none-any.whl size=15617 sha256=e0b758a90622924988986c7a98018451bf05c98ae509cada2e2759a465dd5585\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/45/72/0a/c40b7ee7920915225584a8c7a7822eb503be67c9fa6b93172b\r\n",
      "Successfully built nvidia-cublas-cu11 nvidia-cuda-runtime-cu11 nvidia-cudnn-cu11\r\n",
      "Installing collected packages: nvidia-cudnn-cu116, nvidia-cuda-runtime-cu117, nvidia-cublas-cu117, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia_tensorrt\r\n",
      "Successfully installed nvidia-cublas-cu11-2022.4.8 nvidia-cublas-cu117-11.10.1.25 nvidia-cuda-runtime-cu11-2022.4.25 nvidia-cuda-runtime-cu117-11.7.60 nvidia-cudnn-cu11-2022.5.19 nvidia-cudnn-cu116-8.4.0.27 nvidia_tensorrt-8.4.3.1\r\n",
      "Processing /kaggle/input/torch-tensorrt-pkg/torch_tensorrt-1.2.0-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: torch<1.13.0,>=1.12.0+cu116 in /opt/conda/lib/python3.7/site-packages (from torch-tensorrt==1.2.0) (1.12.1+cu113)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch<1.13.0,>=1.12.0+cu116->torch-tensorrt==1.2.0) (4.1.1)\r\n",
      "Installing collected packages: torch-tensorrt\r\n",
      "Successfully installed torch-tensorrt-1.2.0\r\n",
      "/kaggle/tmp/libs/torch2trt\n",
      "running install\r\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\r\n",
      "  setuptools.SetuptoolsDeprecationWarning,\r\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/easy_install.py:159: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\r\n",
      "  EasyInstallDeprecationWarning,\r\n",
      "running bdist_egg\r\n",
      "running egg_info\r\n",
      "creating torch2trt.egg-info\r\n",
      "writing torch2trt.egg-info/PKG-INFO\r\n",
      "writing dependency_links to torch2trt.egg-info/dependency_links.txt\r\n",
      "writing top-level names to torch2trt.egg-info/top_level.txt\r\n",
      "writing manifest file 'torch2trt.egg-info/SOURCES.txt'\r\n",
      "reading manifest file 'torch2trt.egg-info/SOURCES.txt'\r\n",
      "adding license file 'LICENSE.md'\r\n",
      "writing manifest file 'torch2trt.egg-info/SOURCES.txt'\r\n",
      "installing library code to build/bdist.linux-x86_64/egg\r\n",
      "running install_lib\r\n",
      "running build_py\r\n",
      "creating build\r\n",
      "creating build/lib\r\n",
      "creating build/lib/torch2trt\r\n",
      "copying torch2trt/test.py -> build/lib/torch2trt\r\n",
      "copying torch2trt/torch2trt.py -> build/lib/torch2trt\r\n",
      "copying torch2trt/__init__.py -> build/lib/torch2trt\r\n",
      "copying torch2trt/flatten_module_test.py -> build/lib/torch2trt\r\n",
      "copying torch2trt/module_test.py -> build/lib/torch2trt\r\n",
      "copying torch2trt/dataset_calibrator_test.py -> build/lib/torch2trt\r\n",
      "copying torch2trt/dataset_calibrator.py -> build/lib/torch2trt\r\n",
      "copying torch2trt/dataset.py -> build/lib/torch2trt\r\n",
      "copying torch2trt/dynamic_shape_test.py -> build/lib/torch2trt\r\n",
      "copying torch2trt/flattener_test.py -> build/lib/torch2trt\r\n",
      "copying torch2trt/flatten_module.py -> build/lib/torch2trt\r\n",
      "copying torch2trt/dataset_test.py -> build/lib/torch2trt\r\n",
      "copying torch2trt/utils.py -> build/lib/torch2trt\r\n",
      "copying torch2trt/flattener.py -> build/lib/torch2trt\r\n",
      "creating build/lib/torch2trt/tests\r\n",
      "copying torch2trt/tests/test_tensor_ne.py -> build/lib/torch2trt/tests\r\n",
      "copying torch2trt/tests/__init__.py -> build/lib/torch2trt/tests\r\n",
      "copying torch2trt/tests/test_contiguous.py -> build/lib/torch2trt/tests\r\n",
      "copying torch2trt/tests/test_legacy_max_batch_size.py -> build/lib/torch2trt/tests\r\n",
      "copying torch2trt/tests/test_flatten_dynamic.py -> build/lib/torch2trt/tests\r\n",
      "copying torch2trt/tests/test_tensor_shape_div_batch.py -> build/lib/torch2trt/tests\r\n",
      "copying torch2trt/tests/test_interpolate_dynamic.py -> build/lib/torch2trt/tests\r\n",
      "copying torch2trt/tests/test_tensor_shape.py -> build/lib/torch2trt/tests\r\n",
      "creating build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/BatchNorm2d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/prod.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/Conv1d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/sub.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/layer_norm.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/div.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/permute.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/batch_norm.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/getitem.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/identity.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/__init__.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/AdaptiveAvgPool2d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/view.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/adaptive_max_pool3d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/chunk.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/ne.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/avg_pool.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/mul.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/pow.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/sum.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/normalize.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/max.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/BatchNorm3d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/matmul.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/conv_functional.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/clamp.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/example_plugin.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/gelu.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/interpolate.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/relu6.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/unsqueeze.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/mod.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/sigmoid.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/ConvTranspose2d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/prelu.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/relu.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/compare.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/split.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/tensor.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/pad.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/activation.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/stack.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/silu.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/squeeze.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/expand.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/max_pool1d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/einsum.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/mean.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/Linear.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/AdaptiveAvgPool3d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/min.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/adaptive_max_pool2d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/adaptive_avg_pool2d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/Conv2d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/group_norm.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/BatchNorm1d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/reflection_pad_2d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/dummy_converters.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/softmax.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/adaptive_avg_pool3d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/cat.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/Conv.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/add.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/instance_norm.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/roll.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/unary.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/tanh.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/max_pool3d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/getitem_test.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/clone.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/narrow.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/ConvTranspose.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/LogSoftmax.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/flatten.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/max_pool2d.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/transpose.py -> build/lib/torch2trt/converters\r\n",
      "copying torch2trt/converters/floordiv.py -> build/lib/torch2trt/converters\r\n",
      "creating build/lib/torch2trt/contrib\r\n",
      "copying torch2trt/contrib/__init__.py -> build/lib/torch2trt/contrib\r\n",
      "creating build/lib/torch2trt/tests/timm\r\n",
      "copying torch2trt/tests/timm/__init__.py -> build/lib/torch2trt/tests/timm\r\n",
      "copying torch2trt/tests/timm/test_maxvit.py -> build/lib/torch2trt/tests/timm\r\n",
      "creating build/lib/torch2trt/tests/torchvision\r\n",
      "copying torch2trt/tests/torchvision/save_load.py -> build/lib/torch2trt/tests/torchvision\r\n",
      "copying torch2trt/tests/torchvision/__init__.py -> build/lib/torch2trt/tests/torchvision\r\n",
      "copying torch2trt/tests/torchvision/segmentation.py -> build/lib/torch2trt/tests/torchvision\r\n",
      "copying torch2trt/tests/torchvision/classification.py -> build/lib/torch2trt/tests/torchvision\r\n",
      "creating build/lib/torch2trt/contrib/qat\r\n",
      "copying torch2trt/contrib/qat/__init__.py -> build/lib/torch2trt/contrib/qat\r\n",
      "creating build/lib/torch2trt/contrib/qat/converters\r\n",
      "copying torch2trt/contrib/qat/converters/__init__.py -> build/lib/torch2trt/contrib/qat/converters\r\n",
      "copying torch2trt/contrib/qat/converters/QuantConvBN.py -> build/lib/torch2trt/contrib/qat/converters\r\n",
      "copying torch2trt/contrib/qat/converters/QuantConv.py -> build/lib/torch2trt/contrib/qat/converters\r\n",
      "copying torch2trt/contrib/qat/converters/QuantRelu.py -> build/lib/torch2trt/contrib/qat/converters\r\n",
      "creating build/lib/torch2trt/contrib/qat/layers\r\n",
      "copying torch2trt/contrib/qat/layers/quant_activation.py -> build/lib/torch2trt/contrib/qat/layers\r\n",
      "copying torch2trt/contrib/qat/layers/__init__.py -> build/lib/torch2trt/contrib/qat/layers\r\n",
      "copying torch2trt/contrib/qat/layers/quant_conv.py -> build/lib/torch2trt/contrib/qat/layers\r\n",
      "copying torch2trt/contrib/qat/layers/_utils.py -> build/lib/torch2trt/contrib/qat/layers\r\n",
      "creating build/bdist.linux-x86_64\r\n",
      "creating build/bdist.linux-x86_64/egg\r\n",
      "creating build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "copying build/lib/torch2trt/test.py -> build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "copying build/lib/torch2trt/torch2trt.py -> build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "copying build/lib/torch2trt/__init__.py -> build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "copying build/lib/torch2trt/flatten_module_test.py -> build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "copying build/lib/torch2trt/module_test.py -> build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "creating build/bdist.linux-x86_64/egg/torch2trt/tests\r\n",
      "copying build/lib/torch2trt/tests/test_tensor_ne.py -> build/bdist.linux-x86_64/egg/torch2trt/tests\r\n",
      "copying build/lib/torch2trt/tests/__init__.py -> build/bdist.linux-x86_64/egg/torch2trt/tests\r\n",
      "copying build/lib/torch2trt/tests/test_contiguous.py -> build/bdist.linux-x86_64/egg/torch2trt/tests\r\n",
      "copying build/lib/torch2trt/tests/test_legacy_max_batch_size.py -> build/bdist.linux-x86_64/egg/torch2trt/tests\r\n",
      "copying build/lib/torch2trt/tests/test_flatten_dynamic.py -> build/bdist.linux-x86_64/egg/torch2trt/tests\r\n",
      "copying build/lib/torch2trt/tests/test_tensor_shape_div_batch.py -> build/bdist.linux-x86_64/egg/torch2trt/tests\r\n",
      "creating build/bdist.linux-x86_64/egg/torch2trt/tests/timm\r\n",
      "copying build/lib/torch2trt/tests/timm/__init__.py -> build/bdist.linux-x86_64/egg/torch2trt/tests/timm\r\n",
      "copying build/lib/torch2trt/tests/timm/test_maxvit.py -> build/bdist.linux-x86_64/egg/torch2trt/tests/timm\r\n",
      "copying build/lib/torch2trt/tests/test_interpolate_dynamic.py -> build/bdist.linux-x86_64/egg/torch2trt/tests\r\n",
      "creating build/bdist.linux-x86_64/egg/torch2trt/tests/torchvision\r\n",
      "copying build/lib/torch2trt/tests/torchvision/save_load.py -> build/bdist.linux-x86_64/egg/torch2trt/tests/torchvision\r\n",
      "copying build/lib/torch2trt/tests/torchvision/__init__.py -> build/bdist.linux-x86_64/egg/torch2trt/tests/torchvision\r\n",
      "copying build/lib/torch2trt/tests/torchvision/segmentation.py -> build/bdist.linux-x86_64/egg/torch2trt/tests/torchvision\r\n",
      "copying build/lib/torch2trt/tests/torchvision/classification.py -> build/bdist.linux-x86_64/egg/torch2trt/tests/torchvision\r\n",
      "copying build/lib/torch2trt/tests/test_tensor_shape.py -> build/bdist.linux-x86_64/egg/torch2trt/tests\r\n",
      "copying build/lib/torch2trt/dataset_calibrator_test.py -> build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "creating build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/BatchNorm2d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/prod.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/Conv1d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/sub.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/layer_norm.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/div.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/permute.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/batch_norm.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/getitem.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/identity.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/__init__.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/AdaptiveAvgPool2d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/view.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/adaptive_max_pool3d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/chunk.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/ne.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/avg_pool.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/mul.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/pow.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/sum.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/normalize.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/max.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/BatchNorm3d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/matmul.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/conv_functional.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/clamp.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/example_plugin.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/gelu.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/interpolate.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/relu6.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/unsqueeze.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/mod.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/sigmoid.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/ConvTranspose2d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/prelu.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/relu.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/compare.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/split.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/tensor.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/pad.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/activation.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/stack.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/silu.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/squeeze.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/expand.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/max_pool1d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/einsum.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/mean.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/Linear.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/AdaptiveAvgPool3d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/min.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/adaptive_max_pool2d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/adaptive_avg_pool2d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/Conv2d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/group_norm.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/BatchNorm1d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/reflection_pad_2d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/dummy_converters.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/softmax.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/adaptive_avg_pool3d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/cat.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/Conv.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/add.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/instance_norm.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/roll.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/unary.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/tanh.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/max_pool3d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/getitem_test.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/clone.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/narrow.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/ConvTranspose.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/LogSoftmax.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/flatten.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/max_pool2d.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/transpose.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/converters/floordiv.py -> build/bdist.linux-x86_64/egg/torch2trt/converters\r\n",
      "copying build/lib/torch2trt/dataset_calibrator.py -> build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "copying build/lib/torch2trt/dataset.py -> build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "copying build/lib/torch2trt/dynamic_shape_test.py -> build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "copying build/lib/torch2trt/flattener_test.py -> build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "copying build/lib/torch2trt/flatten_module.py -> build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "copying build/lib/torch2trt/dataset_test.py -> build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "creating build/bdist.linux-x86_64/egg/torch2trt/contrib\r\n",
      "copying build/lib/torch2trt/contrib/__init__.py -> build/bdist.linux-x86_64/egg/torch2trt/contrib\r\n",
      "creating build/bdist.linux-x86_64/egg/torch2trt/contrib/qat\r\n",
      "copying build/lib/torch2trt/contrib/qat/__init__.py -> build/bdist.linux-x86_64/egg/torch2trt/contrib/qat\r\n",
      "creating build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/converters\r\n",
      "copying build/lib/torch2trt/contrib/qat/converters/__init__.py -> build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/converters\r\n",
      "copying build/lib/torch2trt/contrib/qat/converters/QuantConvBN.py -> build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/converters\r\n",
      "copying build/lib/torch2trt/contrib/qat/converters/QuantConv.py -> build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/converters\r\n",
      "copying build/lib/torch2trt/contrib/qat/converters/QuantRelu.py -> build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/converters\r\n",
      "creating build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/layers\r\n",
      "copying build/lib/torch2trt/contrib/qat/layers/quant_activation.py -> build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/layers\r\n",
      "copying build/lib/torch2trt/contrib/qat/layers/__init__.py -> build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/layers\r\n",
      "copying build/lib/torch2trt/contrib/qat/layers/quant_conv.py -> build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/layers\r\n",
      "copying build/lib/torch2trt/contrib/qat/layers/_utils.py -> build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/layers\r\n",
      "copying build/lib/torch2trt/utils.py -> build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "copying build/lib/torch2trt/flattener.py -> build/bdist.linux-x86_64/egg/torch2trt\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/test.py to test.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/torch2trt.py to torch2trt.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/__init__.py to __init__.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/flatten_module_test.py to flatten_module_test.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/module_test.py to module_test.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/tests/test_tensor_ne.py to test_tensor_ne.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/tests/__init__.py to __init__.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/tests/test_contiguous.py to test_contiguous.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/tests/test_legacy_max_batch_size.py to test_legacy_max_batch_size.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/tests/test_flatten_dynamic.py to test_flatten_dynamic.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/tests/test_tensor_shape_div_batch.py to test_tensor_shape_div_batch.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/tests/timm/__init__.py to __init__.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/tests/timm/test_maxvit.py to test_maxvit.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/tests/test_interpolate_dynamic.py to test_interpolate_dynamic.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/tests/torchvision/save_load.py to save_load.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/tests/torchvision/__init__.py to __init__.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/tests/torchvision/segmentation.py to segmentation.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/tests/torchvision/classification.py to classification.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/tests/test_tensor_shape.py to test_tensor_shape.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/dataset_calibrator_test.py to dataset_calibrator_test.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/BatchNorm2d.py to BatchNorm2d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/prod.py to prod.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/Conv1d.py to Conv1d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/sub.py to sub.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/layer_norm.py to layer_norm.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/div.py to div.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/permute.py to permute.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/batch_norm.py to batch_norm.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/getitem.py to getitem.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/identity.py to identity.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/__init__.py to __init__.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/AdaptiveAvgPool2d.py to AdaptiveAvgPool2d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/view.py to view.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/adaptive_max_pool3d.py to adaptive_max_pool3d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/chunk.py to chunk.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/ne.py to ne.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/avg_pool.py to avg_pool.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/mul.py to mul.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/pow.py to pow.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/sum.py to sum.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/normalize.py to normalize.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/max.py to max.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/BatchNorm3d.py to BatchNorm3d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/matmul.py to matmul.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/conv_functional.py to conv_functional.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/clamp.py to clamp.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/example_plugin.py to example_plugin.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/gelu.py to gelu.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/interpolate.py to interpolate.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/relu6.py to relu6.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/unsqueeze.py to unsqueeze.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/mod.py to mod.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/sigmoid.py to sigmoid.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/ConvTranspose2d.py to ConvTranspose2d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/prelu.py to prelu.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/relu.py to relu.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/compare.py to compare.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/split.py to split.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/tensor.py to tensor.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/pad.py to pad.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/activation.py to activation.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/stack.py to stack.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/silu.py to silu.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/squeeze.py to squeeze.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/expand.py to expand.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/max_pool1d.py to max_pool1d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/einsum.py to einsum.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/mean.py to mean.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/Linear.py to Linear.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/AdaptiveAvgPool3d.py to AdaptiveAvgPool3d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/min.py to min.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/adaptive_max_pool2d.py to adaptive_max_pool2d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/adaptive_avg_pool2d.py to adaptive_avg_pool2d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/Conv2d.py to Conv2d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/group_norm.py to group_norm.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/BatchNorm1d.py to BatchNorm1d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/reflection_pad_2d.py to reflection_pad_2d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/dummy_converters.py to dummy_converters.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/softmax.py to softmax.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/adaptive_avg_pool3d.py to adaptive_avg_pool3d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/cat.py to cat.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/Conv.py to Conv.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/add.py to add.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/instance_norm.py to instance_norm.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/roll.py to roll.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/unary.py to unary.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/tanh.py to tanh.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/max_pool3d.py to max_pool3d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/getitem_test.py to getitem_test.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/clone.py to clone.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/narrow.py to narrow.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/ConvTranspose.py to ConvTranspose.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/LogSoftmax.py to LogSoftmax.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/flatten.py to flatten.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/max_pool2d.py to max_pool2d.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/transpose.py to transpose.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/converters/floordiv.py to floordiv.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/dataset_calibrator.py to dataset_calibrator.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/dataset.py to dataset.cpython-37.pyc\r\n",
      "build/bdist.linux-x86_64/egg/torch2trt/dataset.py:61: SyntaxWarning: assertion is always true, perhaps remove parentheses?\r\n",
      "  assert(len(self) > 0, 'Cannot create default flattener without input data.')\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/dynamic_shape_test.py to dynamic_shape_test.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/flattener_test.py to flattener_test.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/flatten_module.py to flatten_module.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/dataset_test.py to dataset_test.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/contrib/__init__.py to __init__.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/__init__.py to __init__.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/converters/__init__.py to __init__.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/converters/QuantConvBN.py to QuantConvBN.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/converters/QuantConv.py to QuantConv.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/converters/QuantRelu.py to QuantRelu.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/layers/quant_activation.py to quant_activation.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/layers/__init__.py to __init__.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/layers/quant_conv.py to quant_conv.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/contrib/qat/layers/_utils.py to _utils.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/utils.py to utils.cpython-37.pyc\r\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch2trt/flattener.py to flattener.cpython-37.pyc\r\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\r\n",
      "copying torch2trt.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\r\n",
      "copying torch2trt.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\r\n",
      "copying torch2trt.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\r\n",
      "copying torch2trt.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\r\n",
      "zip_safe flag not set; analyzing archive contents...\r\n",
      "torch2trt.contrib.qat.layers.__pycache__._utils.cpython-37: module MAY be using inspect.stack\r\n",
      "creating dist\r\n",
      "creating 'dist/torch2trt-0.4.0-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\r\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\r\n",
      "Processing torch2trt-0.4.0-py3.7.egg\r\n",
      "creating /opt/conda/lib/python3.7/site-packages/torch2trt-0.4.0-py3.7.egg\r\n",
      "Extracting torch2trt-0.4.0-py3.7.egg to /opt/conda/lib/python3.7/site-packages\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch2trt-0.4.0-py3.7.egg/torch2trt/dataset.py:61: SyntaxWarning: assertion is always true, perhaps remove parentheses?\r\n",
      "  assert(len(self) > 0, 'Cannot create default flattener without input data.')\r\n",
      "Adding torch2trt 0.4.0 to easy-install.pth file\r\n",
      "\r\n",
      "Installed /opt/conda/lib/python3.7/site-packages/torch2trt-0.4.0-py3.7.egg\r\n",
      "Processing dependencies for torch2trt==0.4.0\r\n",
      "Finished processing dependencies for torch2trt==0.4.0\r\n",
      "Obtaining file:///kaggle/tmp/libs/torch2trt\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hInstalling collected packages: torch2trt\r\n",
      "  Attempting uninstall: torch2trt\r\n",
      "    Found existing installation: torch2trt 0.4.0\r\n",
      "    Uninstalling torch2trt-0.4.0:\r\n",
      "      Successfully uninstalled torch2trt-0.4.0\r\n",
      "  Running setup.py develop for torch2trt\r\n",
      "Successfully installed torch2trt-0.4.0\r\n",
      "/kaggle/working\n",
      "Processing /kaggle/input/kaggle-rsna-pkgs/pylibjpeg-1.4.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pylibjpeg==1.4.0) (1.21.6)\r\n",
      "Installing collected packages: pylibjpeg\r\n",
      "Successfully installed pylibjpeg-1.4.0\r\n",
      "Processing /kaggle/input/kaggle-rsna-pkgs/python_gdcm-3.0.21-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: python-gdcm\r\n",
      "Successfully installed python-gdcm-3.0.21\r\n",
      "Processing /kaggle/input/kaggle-rsna-pkgs/dicomsdl-0.109.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\r\n",
      "Installing collected packages: dicomsdl\r\n",
      "Successfully installed dicomsdl-0.109.1\r\n",
      "Processing /kaggle/input/kaggle-rsna-pkgs/nvidia_dali_nightly_cuda110-1.23.0.dev20230210-7260679-py3-none-manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from nvidia-dali-nightly-cuda110==1.23.0.dev20230210) (0.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from nvidia-dali-nightly-cuda110==1.23.0.dev20230210) (1.6.3)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->nvidia-dali-nightly-cuda110==1.23.0.dev20230210) (0.37.1)\r\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->nvidia-dali-nightly-cuda110==1.23.0.dev20230210) (1.15.0)\r\n",
      "Installing collected packages: nvidia-dali-nightly-cuda110\r\n",
      "Successfully installed nvidia-dali-nightly-cuda110-1.23.0.dev20230210\r\n",
      "Import done!\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/46288847/how-to-suppress-pip-upgrade-warning\n",
    "!pip config set global.disable-pip-version-check true\n",
    "!pip config set global.root-user-action ignore\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_MODULE_LOADING']='LAZY'\n",
    "\n",
    "!mkdir -p /kaggle/tmp/libs\n",
    "\n",
    "# # upgrade pytorch to 1.12 for torch_tensorrt\n",
    "!pip install /kaggle/input/pytorch112-cu113/{torch-1.12.1+cu113-cp37-cp37m-linux_x86_64.whl,torchvision-0.13.1+cu113-cp37-cp37m-linux_x86_64.whl}\n",
    "\n",
    "# timm 0.8.11.dev0\n",
    "!pip uninstall -y timm\n",
    "!cp -r /kaggle/input/kaggle-rsna-pkgs/timm /kaggle/tmp/libs\n",
    "%cd /kaggle/tmp/libs/timm\n",
    "!pip install -e .\n",
    "%cd /kaggle/working\n",
    "\n",
    "try: \n",
    "    import torch2trt\n",
    "except:\n",
    "    !pip install /kaggle/input/torch-tensorrt-pkg/nvidia_pyindex-1.0.9-py3-none-any.whl\n",
    "    !mkdir -p /tmp/pip/cache/\n",
    "    !cp /kaggle/input/torch-tensorrt-pkg/nvidia-cublas-cu11-2022.4.8.xyz /tmp/pip/cache/nvidia-cublas-cu11-2022.4.8.tar.gz\n",
    "    !cp /kaggle/input/torch-tensorrt-pkg/nvidia-cuda-runtime-cu11-2022.4.25.xyz /tmp/pip/cache/nvidia-cuda-runtime-cu11-2022.4.25.tar.gz\n",
    "    !cp /kaggle/input/torch-tensorrt-pkg/nvidia-cudnn-cu11-2022.5.19.xyz /tmp/pip/cache/nvidia-cudnn-cu11-2022.5.19.tar.gz\n",
    "    !cp /kaggle/input/torch-tensorrt-pkg/nvidia_cublas_cu117-11.10.1.25-py3-none-manylinux1_x86_64.whl /tmp/pip/cache/\n",
    "    !cp /kaggle/input/torch-tensorrt-pkg/nvidia_cuda_runtime_cu117-11.7.60-py3-none-manylinux1_x86_64.whl /tmp/pip/cache/\n",
    "    !cp /kaggle/input/torch-tensorrt-pkg/nvidia_cudnn_cu116-8.4.0.27-py3-none-manylinux1_x86_64.whl /tmp/pip/cache/\n",
    "    !cp /kaggle/input/torch-tensorrt-pkg/nvidia_tensorrt-8.4.3.1-cp37-none-linux_x86_64.whl /tmp/pip/cache/\n",
    "    !pip install --no-index --find-links /tmp/pip/cache/ nvidia_tensorrt\n",
    "    !pip install /kaggle/input/torch-tensorrt-pkg/torch_tensorrt-1.2.0-cp37-cp37m-linux_x86_64.whl\n",
    "    \n",
    "    # install torch2trt\n",
    "    !cp -r /kaggle/input/kaggle-rsna-pkgs/torch2trt /kaggle/tmp/libs\n",
    "    %cd /kaggle/tmp/libs/torch2trt\n",
    "    !python setup.py install\n",
    "    !pip install -e .\n",
    "#     !cmake -B build . && cmake --build build --target install && ldconfig\n",
    "    %cd /kaggle/working/\n",
    "\n",
    "try:\n",
    "    import dicomsdl\n",
    "except:\n",
    "    !pip install /kaggle/input/kaggle-rsna-pkgs/pylibjpeg-1.4.0-py3-none-any.whl\n",
    "    !pip install /kaggle/input/kaggle-rsna-pkgs/python_gdcm-3.0.21-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "    !pip install /kaggle/input/kaggle-rsna-pkgs/dicomsdl-0.109.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
    "\n",
    "try:\n",
    "    import dali\n",
    "except:\n",
    "    !pip install /kaggle/input/kaggle-rsna-pkgs/nvidia_dali_nightly_cuda110-1.23.0.dev20230210-7260679-py3-none-manylinux2014_x86_64.whl\n",
    "\n",
    "# try:\n",
    "#     import nvjpeg2k\n",
    "# except:\n",
    "#     # For NVJPEG2k\n",
    "#     !cp /kaggle/input/kaggle-rsna-pkgs/nvjpeg2k.so ./\n",
    "\n",
    "print('Import done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e9bb777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:14:12.407332Z",
     "iopub.status.busy": "2023-02-27T15:14:12.407016Z",
     "iopub.status.idle": "2023-02-27T15:14:14.802277Z",
     "shell.execute_reply": "2023-02-27T15:14:14.801075Z"
    },
    "papermill": {
     "duration": 2.410317,
     "end_time": "2023-02-27T15:14:14.804997",
     "exception": false,
     "start_time": "2023-02-27T15:14:12.394680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timm version: 0.8.11dev0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/tmp/libs/timm')\n",
    "sys.path.append('/opt/conda/lib/python3.7/site-packages/torch2trt-0.4.0-py3.7.egg')\n",
    "sys.path.append('/kaggle/tmp/libs/torch2trt')\n",
    "import timm\n",
    "import gc\n",
    "print('Timm version:', timm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6752d34a",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2023-02-27T15:14:14.830862Z",
     "iopub.status.busy": "2023-02-27T15:14:14.829923Z",
     "iopub.status.idle": "2023-02-27T15:14:14.844764Z",
     "shell.execute_reply": "2023-02-27T15:14:14.843460Z"
    },
    "papermill": {
     "duration": 0.030209,
     "end_time": "2023-02-27T15:14:14.847403",
     "exception": false,
     "start_time": "2023-02-27T15:14:14.817194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing metrics.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile metrics.py\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (auc, confusion_matrix,\n",
    "                             precision_recall_fscore_support, roc_curve)\n",
    "\n",
    "\n",
    "def pfbeta_np(gts, preds, beta=1):\n",
    "    preds = preds.clip(0, 1.)\n",
    "    y_true_count = gts.sum()\n",
    "    ctp = preds[gts == 1].sum()\n",
    "    cfp = preds[gts == 0].sum()\n",
    "    beta_squared = beta * beta\n",
    "    c_precision = ctp / (ctp + cfp)\n",
    "    c_recall = ctp / y_true_count\n",
    "    if (c_precision > 0 and c_recall > 0):\n",
    "        ret = (1 + beta_squared) * (c_precision * c_recall) / (\n",
    "            beta_squared * c_precision + c_recall)\n",
    "        return ret\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def _compute_fbeta(precision, recall, beta=1.0):\n",
    "    return (1 + beta**2) * precision * recall / (\n",
    "        (beta**2) * precision + recall)\n",
    "\n",
    "\n",
    "def compute_usual_metrics(gts, preds, beta=1.0, sample_weights=None):\n",
    "    \"\"\"Binary prediction only.\"\"\"\n",
    "    cfm = confusion_matrix(gts,\n",
    "                           preds,\n",
    "                           labels=[0, 1],\n",
    "                           sample_weight=sample_weights)\n",
    "\n",
    "    tn, fp, fn, tp = cfm.ravel()\n",
    "    acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    fbeta = _compute_fbeta(precision, recall, beta=beta)\n",
    "    # frr = fp / (fp + tn)\n",
    "    # far = fn / (fn + tp)  # 1 - recall\n",
    "    # bacc_beta = _compute_fbeta(1 - frr, 1 - far, beta=beta)\n",
    "    return {\n",
    "        'acc': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'fbeta': fbeta,\n",
    "        # 'bacc_beta': bacc_beta,\n",
    "        # 'frr': frr,\n",
    "        # 'far': far,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics_over_thresholds(preds,\n",
    "                                    gts,\n",
    "                                    thresholds=np.linspace(0, 1, 101),\n",
    "                                    eps=1e-3):\n",
    "    f1scores = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for t in thresholds:\n",
    "        predict = (preds > t).astype(np.float32)\n",
    "\n",
    "        tp = ((predict >= 0.5) & (gts >= 0.5)).sum()\n",
    "        fp = ((predict >= 0.5) & (gts < 0.5)).sum()\n",
    "        fn = ((predict < 0.5) & (gts >= 0.5)).sum()\n",
    "\n",
    "        r = tp / (tp + fn + eps)\n",
    "        p = tp / (tp + fp + eps)\n",
    "        f1 = 2 * r * p / (r + p + eps)\n",
    "        f1scores.append(f1)\n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "    f1scores = np.array(f1scores)\n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "    return f1scores, precisions, recalls, thresholds\n",
    "\n",
    "\n",
    "def compute_best_metrics(cancer_p, cancer_t):\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(cancer_t, cancer_p)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    f1scores, precisions, recalls, thresholds = compute_metrics_over_thresholds(\n",
    "        cancer_p, cancer_t)\n",
    "    i = f1scores.argmax()\n",
    "    f1score, precision, recall, threshold = f1scores[i], precisions[\n",
    "        i], recalls[i], thresholds[i]\n",
    "\n",
    "    specificity = ((cancer_p < threshold) &\n",
    "                   ((cancer_t <= 0.5))).sum() / (cancer_t <= 0.5).sum()\n",
    "    sensitivity = ((cancer_p >= threshold) &\n",
    "                   ((cancer_t >= 0.5))).sum() / (cancer_t >= 0.5).sum()\n",
    "\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'threshold': threshold,\n",
    "        'f1score': f1score,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "    }\n",
    "\n",
    "\n",
    "def print_all_metric(valid_df):\n",
    "\n",
    "    print(\n",
    "        f'{\"    \": <16}    \\tauc      @th     f1      | \tprec    recall  | \tsens    spec '\n",
    "    )\n",
    "    for site_id in [0, 1, 2]:\n",
    "        if site_id > 0:\n",
    "            site_df = valid_df[valid_df.site_id == site_id].reset_index(\n",
    "                drop=True)\n",
    "        else:\n",
    "            site_df = valid_df\n",
    "        # ---\n",
    "\n",
    "        gb = site_df\n",
    "        m = compute_best_metrics(gb.cancer_p, gb.cancer_t)\n",
    "        text = f'{\"single image\": <16} [{site_id}]'\n",
    "        text += f'\\t{m[\"auc\"]:0.5f}'\n",
    "        text += f'\\t{m[\"threshold\"]:0.5f}'\n",
    "        text += f'\\t{m[\"f1score\"]:0.5f} | '\n",
    "        text += f'\\t{m[\"precision\"]:0.5f}'\n",
    "        text += f'\\t{m[\"recall\"]:0.5f} | '\n",
    "        text += f'\\t{m[\"sensitivity\"]:0.5f}'\n",
    "        text += f'\\t{m[\"specificity\"]:0.5f}'\n",
    "        #text += '\\n'\n",
    "        print(text)\n",
    "\n",
    "        # ---\n",
    "\n",
    "        gb = site_df[['patient_id', 'laterality', 'cancer_t',\n",
    "                      'cancer_p']].groupby(['patient_id',\n",
    "                                            'laterality']).mean()\n",
    "        m = compute_best_metrics(gb.cancer_p, gb.cancer_t)\n",
    "        text = f'{\"grouby mean()\": <16} [{site_id}]'\n",
    "        text += f'\\t{m[\"auc\"]:0.5f}'\n",
    "        text += f'\\t{m[\"threshold\"]:0.5f}'\n",
    "        text += f'\\t{m[\"f1score\"]:0.5f} | '\n",
    "        text += f'\\t{m[\"precision\"]:0.5f}'\n",
    "        text += f'\\t{m[\"recall\"]:0.5f} | '\n",
    "        text += f'\\t{m[\"sensitivity\"]:0.5f}'\n",
    "        text += f'\\t{m[\"specificity\"]:0.5f}'\n",
    "        #text += '\\n'\n",
    "        print(text)\n",
    "\n",
    "        # ---\n",
    "        gb = site_df[['patient_id', 'laterality', 'cancer_t',\n",
    "                      'cancer_p']].groupby(['patient_id', 'laterality']).max()\n",
    "        m = compute_best_metrics(gb.cancer_p, gb.cancer_t)\n",
    "        text = f'{\"grouby max()\": <16} [{site_id}]'\n",
    "        text += f'\\t{m[\"auc\"]:0.5f}'\n",
    "        text += f'\\t{m[\"threshold\"]:0.5f}'\n",
    "        text += f'\\t{m[\"f1score\"]:0.5f} | '\n",
    "        text += f'\\t{m[\"precision\"]:0.5f}'\n",
    "        text += f'\\t{m[\"recall\"]:0.5f} | '\n",
    "        text += f'\\t{m[\"sensitivity\"]:0.5f}'\n",
    "        text += f'\\t{m[\"specificity\"]:0.5f}'\n",
    "        #text += '\\n'\n",
    "        print(text)\n",
    "        print(f'--------------\\n')\n",
    "\n",
    "\n",
    "def compute_all(df, plot_save_path):\n",
    "    print(f'Saving plot to {plot_save_path}')\n",
    "    df['cancer_p'] = df['preds']\n",
    "    df['cancer_t'] = df['targets']\n",
    "    print_all_metric(df)\n",
    "\n",
    "    gb = df[['site_id', 'patient_id', 'laterality', 'cancer_t',\n",
    "             'cancer_p']].groupby(['patient_id', 'laterality']).mean()\n",
    "    gb.loc[:, 'cancer_t'] = gb.cancer_t.astype(int)\n",
    "    m = compute_best_metrics(gb.cancer_p, gb.cancer_t)\n",
    "    text = f'{\"grouby mean()\": <16}'\n",
    "    text += f'\\t{m[\"auc\"]:0.5f}'\n",
    "    text += f'\\t{m[\"threshold\"]:0.5f}'\n",
    "    text += f'\\t{m[\"f1score\"]:0.5f} | '\n",
    "    text += f'\\t{m[\"precision\"]:0.5f}'\n",
    "    text += f'\\t{m[\"recall\"]:0.5f} | '\n",
    "    text += f'\\t{m[\"sensitivity\"]:0.5f}'\n",
    "    text += f'\\t{m[\"specificity\"]:0.5f}'\n",
    "    text += '\\n'\n",
    "    print(text)\n",
    "\n",
    "    pfbeta = pfbeta_np(gb.cancer_t.values, gb.cancer_p.values, beta=1)\n",
    "    print('PROBABILITY-FBETA:', pfbeta)\n",
    "\n",
    "    plot_pr_curve(gb, plot_save_path)\n",
    "\n",
    "\n",
    "def plot_pr_curve(df, plot_save_path):\n",
    "    f1scores, precisions, recalls, thresholds = compute_metrics_over_thresholds(\n",
    "        df.cancer_p, df.cancer_t)\n",
    "    i = f1scores.argmax()\n",
    "    f1score_max, precision_max, recall_max, threshold_max = f1scores[\n",
    "        i], precisions[i], recalls[i], thresholds[i]\n",
    "    print(\n",
    "        f'f1score_max = {f1score_max}, precision_max = {precision_max}, recall_max = {recall_max}, threshold_max = {threshold_max}'\n",
    "    )\n",
    "\n",
    "    _, axs = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "    ############################################################################\n",
    "    ### PRECISION-RECALL CURVE\n",
    "    f_scores = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7,\n",
    "                0.8]  #np.linspace(0.2, 0.8, num=8)\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        (l, ) = axs[0, 0].plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "        axs[0, 0].annotate(\"f1={0:0.1f}\".format(f_score),\n",
    "                           xy=(0.9, y[45] + 0.02))\n",
    "    axs[0, 0].plot([0, 1], [0, 1], color=\"gray\", alpha=0.2)\n",
    "\n",
    "    # overall\n",
    "    precision, recall, threshold = metrics.precision_recall_curve(\n",
    "        df.cancer_t, df.cancer_p)\n",
    "    auc = metrics.auc(recall, precision)\n",
    "    axs[0, 0].plot(recall, precision)\n",
    "    s = axs[0, 0].scatter(recall[:-1], precision[:-1], c=threshold, cmap='hsv')\n",
    "    axs[0, 0].scatter(recall_max, precision_max, s=30, c='k')\n",
    "\n",
    "    # for each site\n",
    "    precision, recall, threshold = metrics.precision_recall_curve(\n",
    "        df.cancer_t[df.site_id == 1], df.cancer_p[df.site_id == 1])\n",
    "    axs[0, 0].plot(recall, precision, '--', label='site_id=1')\n",
    "    precision, recall, threshold = metrics.precision_recall_curve(\n",
    "        df.cancer_t[df.site_id == 2], df.cancer_p[df.site_id == 2])\n",
    "    axs[0, 0].plot(recall, precision, '--', label='site_id=2')\n",
    "\n",
    "    axs[0, 0].set_xlim([0.0, 1.0])\n",
    "    axs[0, 0].set_ylim([0.0, 1.05])\n",
    "\n",
    "    text = ''\n",
    "    text += f'MAX f1score {f1score_max: 0.5f} @ th = {threshold_max: 0.5f}\\n'\n",
    "    text += f'prec {precision_max: 0.5f}, recall {recall_max: 0.5f}, pr-auc {auc: 0.5f}\\n'\n",
    "\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].set_title(text)\n",
    "    plt.colorbar(s, ax=axs[0, 0], label='threshold')\n",
    "    axs[0, 0].set_xlabel('recall')\n",
    "    axs[0, 0].set_ylabel('precision')\n",
    "\n",
    "    ############################################################################\n",
    "    # HISTOGRAM\n",
    "    spacing = 51\n",
    "\n",
    "    for site_type in [0, 1, 2]:\n",
    "        if site_type == 0:\n",
    "            ax = axs[0, 1]\n",
    "            sub_df = df\n",
    "            title = 'All site'\n",
    "        elif site_type == 1:\n",
    "            ax = axs[1, 0]\n",
    "            sub_df = df[df.site_id == site_type].reset_index(drop=True)\n",
    "            title = 'Site 1'\n",
    "        elif site_type == 2:\n",
    "            ax = axs[1, 1]\n",
    "            sub_df = df[df.site_id == site_type].reset_index(drop=True)\n",
    "            title = 'Site 2'\n",
    "\n",
    "        cancer_p = sub_df.cancer_p\n",
    "        cancer_t = sub_df.cancer_t\n",
    "        cancer_t = cancer_t.astype(int)\n",
    "        pos, bin = np.histogram(cancer_p[cancer_t == 1],\n",
    "                                np.linspace(0, 1, spacing))\n",
    "        neg, bin = np.histogram(cancer_p[cancer_t == 0],\n",
    "                                np.linspace(0, 1, spacing))\n",
    "        pos = pos / (cancer_t == 1).sum()\n",
    "        neg = neg / (cancer_t == 0).sum()\n",
    "        # plt.plot(bin[1:],neg, alpha=1)\n",
    "        # plt.plot(bin[1:],pos, alpha=1)\n",
    "        bin = (bin[1:] + bin[:-1]) / 2\n",
    "        ax.bar(bin, neg, width=1 / spacing, label='neg', alpha=0.5)\n",
    "        ax.bar(bin, pos, width=1 / spacing, label='pos', alpha=0.5)\n",
    "        ax.legend()\n",
    "        ax.set_title(title)\n",
    "\n",
    "    # plt.show()\n",
    "    plt.savefig(plot_save_path)\n",
    "\n",
    "\n",
    "def _compute_metrics(gts,\n",
    "                     preds,\n",
    "                     sample_weights=None,\n",
    "                     thres_range=(0, 1, 0.01),\n",
    "                     sort_by='pfbeta'):\n",
    "    if isinstance(gts, torch.Tensor):\n",
    "        gts = gts.cpu().numpy()\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.cpu().numpy()\n",
    "    assert isinstance(gts, np.ndarray) and isinstance(preds, np.ndarray)\n",
    "    assert len(preds) == len(gts)\n",
    "\n",
    "    # Probabilistic-fbeta\n",
    "    pfbeta = pfbeta_np(gts, preds, beta=1.0)\n",
    "    # AUC\n",
    "    fpr, tpr, _thresholds = sklearn.metrics.roc_curve(gts, preds, pos_label=1)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "\n",
    "    # PR-AUC\n",
    "    precisions, recalls, _thresholds = sklearn.metrics.precision_recall_curve(\n",
    "        gts, preds)\n",
    "    pr_auc = sklearn.metrics.auc(recalls, precisions)\n",
    "\n",
    "    ##### METRICS FOR CATEGORICAL PREDICTION #####\n",
    "    # PER THRESHOLD METRIC\n",
    "    per_thres_metrics = []\n",
    "    for thres in np.arange(*thres_range):\n",
    "        bin_preds = (preds > thres).astype(np.uint8)\n",
    "        metric_at_thres = compute_usual_metrics(gts, bin_preds, beta=1.0)\n",
    "        pfbeta_at_thres = pfbeta_np(gts, bin_preds, beta=1.0)\n",
    "        metric_at_thres['pfbeta'] = pfbeta_at_thres\n",
    "\n",
    "        if sample_weights is not None:\n",
    "            w_metric_at_thres = compute_usual_metrics(gts, bin_preds, beta=1.0)\n",
    "            w_metric_at_thres = {\n",
    "                f'w_{k}': v\n",
    "                for k, v in w_metric_at_thres.items()\n",
    "            }\n",
    "            metric_at_thres.update(w_metric_at_thres)\n",
    "        per_thres_metrics.append((thres, metric_at_thres))\n",
    "\n",
    "    per_thres_metrics.sort(key=lambda x: x[1][sort_by], reverse=True)\n",
    "\n",
    "    # handle multiple thresholds with same scores\n",
    "    top_score = per_thres_metrics[0][1][sort_by]\n",
    "    same_scores = []\n",
    "    for j, (thres, metric_at_thres) in enumerate(per_thres_metrics):\n",
    "        if metric_at_thres[sort_by] == top_score:\n",
    "            same_scores.append(abs(thres - 0.5))\n",
    "        else:\n",
    "            assert metric_at_thres[sort_by] < top_score\n",
    "            break\n",
    "    if len(same_scores) == 1:\n",
    "        best_thres, best_metric = per_thres_metrics[0]\n",
    "    else:\n",
    "        # the nearer 0.5 threshold is --> better\n",
    "        best_idx = np.argmin(np.array(same_scores))\n",
    "        best_thres, best_metric = per_thres_metrics[best_idx]\n",
    "\n",
    "    # best thres, best results, all results\n",
    "    return {\n",
    "        'best_thres': best_thres,\n",
    "        'best_metric': best_metric,\n",
    "        'all_metrics': per_thres_metrics,\n",
    "        'pfbeta': pfbeta,\n",
    "        'auc': auc,\n",
    "        'prauc': pr_auc,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(df,\n",
    "                    plot_save_path='plot.png',\n",
    "                    thres_range=(0, 1, 0.01),\n",
    "                    sort_by='pfbeta',\n",
    "                    additional_info=False):\n",
    "    ori_df = df[[\n",
    "        'site_id', 'patient_id', 'laterality', 'cancer', 'preds', 'targets'\n",
    "    ]]\n",
    "    all_metrics = {}\n",
    "\n",
    "    reducer_single = lambda df: df\n",
    "    reducer_gbmean = lambda df: df.groupby(['patient_id', 'laterality']).mean()\n",
    "    reducer_gbmax = lambda df: df.groupby(['patient_id', 'laterality']).mean()\n",
    "    reducer_gbmean_site1 = lambda df: df[df.site_id == 1].reset_index(\n",
    "        drop=True).groupby(['patient_id', 'laterality']).mean()\n",
    "    reducer_gbmean_site2 = lambda df: df[df.site_id == 2].reset_index(\n",
    "        drop=True).groupby(['patient_id', 'laterality']).mean()\n",
    "\n",
    "    reducers = {\n",
    "        'single': reducer_single,\n",
    "        'gbmean': reducer_gbmean,\n",
    "        'gbmean_site1': reducer_gbmean_site1,\n",
    "        'gbmean_site2': reducer_gbmean_site2,\n",
    "        'gbmax': reducer_gbmax,\n",
    "    }\n",
    "\n",
    "    for reducer_name, reducer in reducers.items():\n",
    "        df = reducer(ori_df.copy())\n",
    "        preds = df['preds'].to_numpy()\n",
    "        gts = df['targets'].to_numpy()\n",
    "        # mean_sample_weights = mean_df['sample_weights']\n",
    "        _metrics = _compute_metrics(gts, preds, None, thres_range, sort_by)\n",
    "        all_metrics[f'{reducer_name}_best_thres'] = _metrics['best_thres']\n",
    "        all_metrics.update({\n",
    "            f'{reducer_name}_best_{k}': v\n",
    "            for k, v in _metrics['best_metric'].items()\n",
    "        })\n",
    "        all_metrics[f'{reducer_name}_pfbeta'] = _metrics['pfbeta']\n",
    "        all_metrics[f'{reducer_name}_auc'] = _metrics['auc']\n",
    "        all_metrics[f'{reducer_name}_prauc'] = _metrics['prauc']\n",
    "\n",
    "    # rank 0 only\n",
    "    if additional_info:\n",
    "        compute_all(ori_df, plot_save_path)\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669739a2",
   "metadata": {
    "papermill": {
     "duration": 0.010763,
     "end_time": "2023-02-27T15:14:14.869781",
     "exception": false,
     "start_time": "2023-02-27T15:14:14.859018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ROI extraction:\n",
    "\n",
    "YOLOX + (Otsu find contours) fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9acca62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:14:14.894160Z",
     "iopub.status.busy": "2023-02-27T15:14:14.893840Z",
     "iopub.status.idle": "2023-02-27T15:14:14.903773Z",
     "shell.execute_reply": "2023-02-27T15:14:14.902826Z"
    },
    "papermill": {
     "duration": 0.025169,
     "end_time": "2023-02-27T15:14:14.906430",
     "exception": false,
     "start_time": "2023-02-27T15:14:14.881261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing roi_extract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile roi_extract.py\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n",
    "\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "sys.path.append('/kaggle/tmp/libs/')\n",
    "from torch2trt import TRTModule\n",
    "from torch.nn import functional as F\n",
    "\n",
    "_TORCH_VER = [int(x) for x in torch.__version__.split(\".\")[:2]]\n",
    "_TORCH11X = (_TORCH_VER >= [1, 10])\n",
    "\n",
    "\n",
    "def meshgrid(*tensors):\n",
    "    if _TORCH11X:\n",
    "        return torch.meshgrid(*tensors, indexing=\"ij\")\n",
    "    else:\n",
    "        return torch.meshgrid(*tensors)\n",
    "\n",
    "\n",
    "def extract_roi_otsu(img, gkernel=(5, 5)):\n",
    "    \"\"\"WARNING: this function modify input image inplace.\"\"\"\n",
    "    ori_h, ori_w = img.shape[:2]\n",
    "    # clip percentile: implant, white lines\n",
    "    upper = np.percentile(img, 95)\n",
    "    img[img > upper] = np.min(img)\n",
    "    # Gaussian filtering to reduce noise (optional)\n",
    "    if gkernel is not None:\n",
    "        img = cv2.GaussianBlur(img, gkernel, 0)\n",
    "    _, img_bin = cv2.threshold(img, 0, 255,\n",
    "                               cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    # dilation to improve contours connectivity\n",
    "    element = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3), (-1, -1))\n",
    "    img_bin = cv2.dilate(img_bin, element)\n",
    "    cnts, _ = cv2.findContours(img_bin, cv2.RETR_EXTERNAL,\n",
    "                               cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(cnts) == 0:\n",
    "        return None, None, None\n",
    "    areas = np.array([cv2.contourArea(cnt) for cnt in cnts])\n",
    "    select_idx = np.argmax(areas)\n",
    "    cnt = cnts[select_idx]\n",
    "    area_pct = areas[select_idx] / (img.shape[0] * img.shape[1])\n",
    "    x0, y0, w, h = cv2.boundingRect(cnt)\n",
    "    # min-max for safety only\n",
    "    # x0, y0, x1, y1\n",
    "    x1 = min(max(int(x0 + w), 0), ori_w)\n",
    "    y1 = min(max(int(y0 + h), 0), ori_h)\n",
    "    x0 = min(max(int(x0), 0), ori_w)\n",
    "    y0 = min(max(int(y0), 0), ori_h)\n",
    "    return [x0, y0, x1, y1], area_pct, None\n",
    "\n",
    "\n",
    "class RoiExtractor:\n",
    "\n",
    "    def __init__(self,\n",
    "                 engine_path,\n",
    "                 input_size,\n",
    "                 num_classes,\n",
    "                 conf_thres=0.5,\n",
    "                 nms_thres=0.9,\n",
    "                 class_agnostic=False,\n",
    "                 area_pct_thres=0.04,\n",
    "                 hw=None,\n",
    "                 strides=None,\n",
    "                 exp=None):\n",
    "        self.input_size = input_size\n",
    "        self.input_h, self.input_w = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.conf_thres = conf_thres\n",
    "        self.nms_thres = nms_thres\n",
    "        self.class_agnostic = class_agnostic\n",
    "        self.area_pct_thres = area_pct_thres\n",
    "\n",
    "        model = TRTModule()\n",
    "        model.load_state_dict(torch.load(engine_path))\n",
    "        self.model = model\n",
    "        if hw is None or strides is None:\n",
    "            assert exp is not None\n",
    "            self._set_meta(exp)\n",
    "        else:\n",
    "            self.hw = hw\n",
    "            self.strides = strides\n",
    "\n",
    "    def _set_meta(self, exp):\n",
    "        assert exp is not None\n",
    "        print(\"Start probing model metadata..\")\n",
    "        # dummy infer\n",
    "        torch_model = exp.get_model().cuda().eval()\n",
    "        _dummy = torch.ones(1, 3, exp.test_size[0], exp.test_size[1]).cuda()\n",
    "        torch_model(_dummy)\n",
    "        # set attributes\n",
    "        self.hw = torch_model.head.hw\n",
    "        self.strides = torch_model.head.strides\n",
    "        # cleanup\n",
    "        del torch_model, _dummy\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print('Done probbing model metadata..')\n",
    "\n",
    "    def decode_outputs(self, outputs):\n",
    "        dtype = outputs.type()\n",
    "        grids = []\n",
    "        strides = []\n",
    "        for (hsize, wsize), stride in zip(self.hw, self.strides):\n",
    "            yv, xv = meshgrid([torch.arange(hsize), torch.arange(wsize)])\n",
    "            grid = torch.stack((xv, yv), 2).view(1, -1, 2)\n",
    "            grids.append(grid)\n",
    "            shape = grid.shape[:2]\n",
    "            strides.append(torch.full((*shape, 1), stride))\n",
    "\n",
    "        grids = torch.cat(grids, dim=1).type(dtype)\n",
    "        strides = torch.cat(strides, dim=1).type(dtype)\n",
    "\n",
    "        outputs = torch.cat(\n",
    "            [(outputs[..., 0:2] + grids) * strides,\n",
    "             torch.exp(outputs[..., 2:4]) * strides, outputs[..., 4:]],\n",
    "            dim=-1)\n",
    "        return outputs\n",
    "\n",
    "    def post_process(self,\n",
    "                     pred,\n",
    "                     conf_thres=0.5,\n",
    "                     nms_thres=0.9,\n",
    "                     class_agnostic=False):\n",
    "        box_corner = pred.new(pred.shape)\n",
    "        box_corner[:, :, 0] = pred[:, :, 0] - pred[:, :, 2] / 2\n",
    "        box_corner[:, :, 1] = pred[:, :, 1] - pred[:, :, 3] / 2\n",
    "        box_corner[:, :, 2] = pred[:, :, 0] + pred[:, :, 2] / 2\n",
    "        box_corner[:, :, 3] = pred[:, :, 1] + pred[:, :, 3] / 2\n",
    "        pred[:, :, :4] = box_corner[:, :, :4]\n",
    "\n",
    "        output = [None for _ in range(len(pred))]\n",
    "        for i, image_pred in enumerate(pred):\n",
    "\n",
    "            # If none are remaining => process next image\n",
    "            if not image_pred.size(0):\n",
    "                continue\n",
    "            # Get score and class with highest confidence\n",
    "            class_conf, class_pred = torch.max(image_pred[:, 5:5 +\n",
    "                                                          self.num_classes],\n",
    "                                               1,\n",
    "                                               keepdim=True)\n",
    "\n",
    "            conf_mask = (image_pred[:, 4] * class_conf.squeeze() >=\n",
    "                         conf_thres).squeeze()\n",
    "            # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n",
    "            detections = torch.cat(\n",
    "                (image_pred[:, :5], class_conf, class_pred.float()), 1)\n",
    "            detections = detections[conf_mask]\n",
    "            if not detections.size(0):\n",
    "                continue\n",
    "\n",
    "            if class_agnostic:\n",
    "                nms_out_index = torchvision.ops.nms(\n",
    "                    detections[:, :4],\n",
    "                    detections[:, 4] * detections[:, 5],\n",
    "                    nms_thres,\n",
    "                )\n",
    "            else:\n",
    "                nms_out_index = torchvision.ops.batched_nms(\n",
    "                    detections[:, :4],\n",
    "                    detections[:, 4] * detections[:, 5],\n",
    "                    detections[:, 6],\n",
    "                    nms_thres,\n",
    "                )\n",
    "            detections = detections[nms_out_index]\n",
    "            if output[i] is None:\n",
    "                output[i] = detections\n",
    "            else:\n",
    "                output[i] = torch.cat((output[i], detections))\n",
    "        return output\n",
    "\n",
    "    def preprocess_single(self, img: torch.Tensor):\n",
    "        ori_h = img.size(0)\n",
    "        ori_w = img.size(1)\n",
    "        ratio = min(self.input_h / ori_h, self.input_w / ori_w)\n",
    "        # resize\n",
    "        resized_img = F.interpolate(img.view(1, 1, ori_h, ori_w),\n",
    "                                    mode=\"bilinear\",\n",
    "                                    scale_factor=ratio,\n",
    "                                    recompute_scale_factor=True)[0, 0]\n",
    "        # padding\n",
    "        padded_img = torch.full((self.input_h, self.input_w),\n",
    "                                114,\n",
    "                                dtype=resized_img.dtype,\n",
    "                                device='cuda')\n",
    "        padded_img[:resized_img.size(0), :resized_img.size(1)] = resized_img\n",
    "        # 1 channel --> 3 channels\n",
    "        padded_img = padded_img.unsqueeze(-1).expand(-1, -1, 3)\n",
    "        # HWC --> CHW\n",
    "        padded_img = padded_img.permute(2, 0, 1)\n",
    "        padded_img = padded_img.float()\n",
    "        return padded_img, resized_img, ratio, ori_h, ori_w\n",
    "\n",
    "    def detect_single(self, img):\n",
    "        padded_img, resized_img, ratio, ori_h, ori_w = self.preprocess_single(\n",
    "            img)\n",
    "        padded_img = padded_img.unsqueeze(0)\n",
    "        output = self.model(padded_img)\n",
    "        output = self.decode_outputs(output)\n",
    "        # x0, y0, x1, y1, box_conf, cls_conf, cls_id\n",
    "        output = self.post_process(output, self.conf_thres, self.nms_thres)[0]\n",
    "        if output is not None:\n",
    "            output[:, :4] = output[:, :4] / ratio\n",
    "            # re-compute: conf = box_conf * cls_conf\n",
    "            output[:, 4] = output[:, 4] * output[:, 5]\n",
    "            # select box with highest confident\n",
    "            output = output[output[:, 4].argmax()]\n",
    "            x0 = min(max(int(output[0]), 0), ori_w)\n",
    "            y0 = min(max(int(output[1]), 0), ori_h)\n",
    "            x1 = min(max(int(output[2]), 0), ori_w)\n",
    "            y1 = min(max(int(output[3]), 0), ori_h)\n",
    "            area_pct = (x1 - x0) * (y1 - y0) / (ori_h * ori_w)\n",
    "            if area_pct >= self.area_pct_thres:\n",
    "                # xyxy, area_pct, conf\n",
    "                return [x0, y0, x1, y1], area_pct, output[4]\n",
    "\n",
    "        # if YOLOX fail, try Otsu thresholding + find contours\n",
    "        xyxy, area_pct, _ = extract_roi_otsu(\n",
    "            resized_img.to(torch.uint8).cpu().numpy())\n",
    "        # if both fail, use full frame\n",
    "        if xyxy is not None:\n",
    "            if area_pct >= self.area_pct_thres:\n",
    "                print('ROI detection: using Otsu.')\n",
    "                x0, y0, x1, y1 = xyxy\n",
    "                x0 = min(max(int(x0 / ratio), 0), ori_w)\n",
    "                y0 = min(max(int(y0 / ratio), 0), ori_h)\n",
    "                x1 = min(max(int(x1 / ratio), 0), ori_w)\n",
    "                y1 = min(max(int(y1 / ratio), 0), ori_h)\n",
    "                return [x0, y0, x1, y1], area_pct, None\n",
    "        print('ROI detection: both fail.')\n",
    "        return None, area_pct, None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40ea521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:14:14.931574Z",
     "iopub.status.busy": "2023-02-27T15:14:14.930722Z",
     "iopub.status.idle": "2023-02-27T15:14:14.941767Z",
     "shell.execute_reply": "2023-02-27T15:14:14.940819Z"
    },
    "papermill": {
     "duration": 0.025945,
     "end_time": "2023-02-27T15:14:14.944416",
     "exception": false,
     "start_time": "2023-02-27T15:14:14.918471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing convert_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile convert_model.py\n",
    "\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch2trt import torch2trt as torch2trt\n",
    "from torch2trt import TRTModule\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch_tensorrt\n",
    "\n",
    "# sys.path.append('/home/dangnh36/projects/.comp/rsna/src/pytorch-image-models/')\n",
    "from timm.data import resolve_data_config\n",
    "from timm.models import create_model\n",
    "\n",
    "# MODEL_NAME = 'tf_efficientnetv2_s.in21k_ft_in1k'\n",
    "# MODEL_CKPTS = [\n",
    "#     '/kaggle/input/kaggle-rsna-ckpts/fold0_ep12_auc8989_pf5090_pr4709_th050.pth.tar',\n",
    "#     '/kaggle/input/kaggle-rsna-ckpts/fold1_ep20_auc8591_pf5028_pr4423_th026.pth.tar',\n",
    "#     '/kaggle/input/kaggle-rsna-ckpts/fold2_ep20_auc8854_pf4262_pr3749_th029.pth.tar',\n",
    "#     '/kaggle/input/kaggle-rsna-ckpts/fold3_ep15_auc8839_pf4571_pr4215_th020.pth.tar',\n",
    "#     '/kaggle/input/kaggle-rsna-ckpts/fold4_ep15_auc9071_pf5030_pr4445_th037.pth.tar',\n",
    "    \n",
    "# ]\n",
    "# GLOBAL_POOL = 'max'\n",
    "# NUM_CLASSES = 2\n",
    "# IN_CHANS = 3\n",
    "# # MEAN = np.array([0.485, 0.456, 0.406]) * 255\n",
    "# # STD = np.array([0.229, 0.224, 0.225]) * 255\n",
    "# MEAN = np.array([0.5, 0.5, 0.5]) * 255\n",
    "# STD = np.array([0.5, 0.5, 0.5]) * 255\n",
    "\n",
    "\n",
    "MODEL_NAME = 'convnext_small.fb_in22k_ft_in1k_384'\n",
    "MODEL_CKPTS = [\n",
    "    '/kaggle/input/kaggle-rsna-ckpts/final_fold0_ep11.pth.tar',\n",
    "    '/kaggle/input/kaggle-rsna-ckpts/final_fold1_ep8.pth.tar',\n",
    "    '/kaggle/input/kaggle-rsna-ckpts/final_fold2_ep26.pth.tar',\n",
    "    '/kaggle/input/kaggle-rsna-ckpts/final_fold3_ep19.pth.tar',\n",
    "]\n",
    "GLOBAL_POOL = 'max'\n",
    "NUM_CLASSES = 1\n",
    "IN_CHANS = 3\n",
    "MEAN = np.array([0.485, 0.456, 0.406]) * 255\n",
    "STD = np.array([0.229, 0.224, 0.225]) * 255\n",
    "\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "PRECISION = 'fp32'\n",
    "HALF_INPUT = False\n",
    "# TRT_BACKEND = 'torch_tensorrt'\n",
    "TRT_BACKEND = 'torch2trt'\n",
    "assert TRT_BACKEND in ['torch_tensorrt', 'torch2trt']\n",
    "TRT_SAVE_PATH = f'{TRT_BACKEND}_{MODEL_NAME.split(\".\")[0]}_b{BATCH_SIZE}_{PRECISION}.engine'\n",
    "\n",
    "\n",
    "class KFoldEnsembleModel(nn.Module):\n",
    "\n",
    "    def __init__(self, model_name, num_classes, in_chans, ckpt_paths):\n",
    "        super(KFoldEnsembleModel, self).__init__()\n",
    "        fmodels = []\n",
    "        for i, ckpt_path in enumerate(ckpt_paths):\n",
    "            fmodel = create_model(\n",
    "                model_name,\n",
    "                num_classes=num_classes,\n",
    "                in_chans=in_chans,\n",
    "                pretrained=False,\n",
    "                checkpoint_path=ckpt_path,\n",
    "                global_pool=GLOBAL_POOL,\n",
    "            ).eval()\n",
    "            # print(fmodel)\n",
    "            data_config = resolve_data_config({}, model=fmodel)\n",
    "            print('Data config:', data_config)\n",
    "            print(MEAN, STD)\n",
    "            fmodels.append(fmodel)\n",
    "        self.fmodels = nn.ModuleList(fmodels)\n",
    "\n",
    "        self.register_buffer('mean',\n",
    "                             torch.FloatTensor(MEAN).reshape(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.FloatTensor(STD).reshape(1, 3, 1, 1))\n",
    "\n",
    "#     def forward(self, x):\n",
    "# #         x = x.sub(self.mean).div(self.std)\n",
    "# #         # NHWC --> NCHW\n",
    "# #         x = x.permute(0, 3, 1, 2)\n",
    "#         x = (x - self.mean) / self.std\n",
    "#         x_flip = torch.flip(x, [3])\n",
    "#         # 2N * C * H * W\n",
    "#         x = torch.cat([x, x_flip], dim=0)\n",
    "#         probs = []\n",
    "#         for fmodel in self.fmodels:\n",
    "#             logits = fmodel(x)\n",
    "# #             prob = logits.softmax(dim=1)[:, 1]\n",
    "#             prob = logits.sigmoid()[:, 0]\n",
    "#             probs.append(prob)\n",
    "#         probs = torch.stack(probs, dim=1)\n",
    "#         return probs\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.sub(self.mean).div(self.std)\n",
    "        x = (x - self.mean) / self.std\n",
    "        probs = []\n",
    "        for fmodel in self.fmodels:\n",
    "            logits = fmodel(x)\n",
    "#             prob = logits.softmax(dim=1)[:, 1]\n",
    "            prob = logits.sigmoid()[:, 0]\n",
    "            probs.append(prob)\n",
    "        probs = torch.stack(probs, dim=1)\n",
    "        return probs\n",
    "\n",
    "\n",
    "def get_sample_batch():\n",
    "    return (torch.rand(BATCH_SIZE, 3, 2048, 1024) * 255).float().cuda()\n",
    "    img_dir = '/kaggle/tmp/pngs'\n",
    "    img_names = os.listdir(img_dir)\n",
    "    imgs = []\n",
    "    for img_name in img_names[:BATCH_SIZE]:\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        imgs.append(img)\n",
    "    # B H W 3\n",
    "    batch = np.stack(imgs, axis=0)\n",
    "    batch = torch.from_numpy(batch)\n",
    "    batch = batch.permute(0, 3, 1, 2)\n",
    "    batch = batch.cuda().float()\n",
    "    return batch\n",
    "\n",
    "\n",
    "def convert_with_torch2trt():\n",
    "    if PRECISION=='fp32':\n",
    "        use_fp16 = False\n",
    "    elif PRECISION == 'fp16':\n",
    "        use_fp16 = True\n",
    "    else:\n",
    "        raise AssertionError()\n",
    "        \n",
    "    model = KFoldEnsembleModel(MODEL_NAME, NUM_CLASSES, IN_CHANS, MODEL_CKPTS)\n",
    "    # print(model)\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    sample_batch = get_sample_batch()\n",
    "    print('Sample input shape:', sample_batch.shape)\n",
    "    with torch.inference_mode():\n",
    "        prob = model(sample_batch)\n",
    "        print(prob.shape, prob)\n",
    "\n",
    "    # CONVERT TO TENSORRT\n",
    "    with torch.inference_mode():\n",
    "        print('START CONVERT')\n",
    "        model_trt = torch2trt(\n",
    "            model,\n",
    "            inputs=[sample_batch],\n",
    "            input_names=None,\n",
    "            output_names=None,\n",
    "            log_level=trt.Logger.ERROR,\n",
    "            fp16_mode=use_fp16,\n",
    "            max_workspace_size=1 << 32,\n",
    "            strict_type_constraints=False,\n",
    "            keep_network=True,\n",
    "            use_onnx=True,\n",
    "            default_device_type=trt.DeviceType.GPU,\n",
    "            dla_core=0,\n",
    "            gpu_fallback=True,\n",
    "            device_types={},\n",
    "            min_shapes=[(1, 3, 2048, 1024)],\n",
    "            max_shapes=[(BATCH_SIZE, 3, 2048, 1024)],\n",
    "            opt_shapes=[(BATCH_SIZE, 3, 2048, 1024)],\n",
    "            onnx_opset=15,\n",
    "            max_batch_size=BATCH_SIZE,\n",
    "        )\n",
    "    torch.save(model_trt.state_dict(), TRT_SAVE_PATH)\n",
    "    print('ALL DONE!')\n",
    "    \n",
    "    \n",
    "\n",
    "def convert_with_torch_tensorrt():\n",
    "    if PRECISION=='fp32':\n",
    "        use_fp16 = False\n",
    "    elif PRECISION == 'fp16':\n",
    "        use_fp16 = True\n",
    "    else:\n",
    "        raise AssertionError()\n",
    "        \n",
    "    model = KFoldEnsembleModel(MODEL_NAME, NUM_CLASSES, IN_CHANS, MODEL_CKPTS)\n",
    "    # print(model)\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    \n",
    "    inputs = [\n",
    "        torch_tensorrt.Input(\n",
    "            min_shape=(1, 3, 2048, 1024),\n",
    "            opt_shape=(BATCH_SIZE, 3, 2048, 1024),\n",
    "            max_shape=(BATCH_SIZE, 3, 2048, 1024),\n",
    "        dtype=torch.half if HALF_INPUT else torch.float32,\n",
    "#         format=torch_tensorrt.TensorFormat.NCHW\n",
    "    )]\n",
    "    \n",
    "    if use_fp16:\n",
    "        enable_precisions = torch.half\n",
    "    else:\n",
    "        enable_precisions = torch.float32\n",
    "        \n",
    "#     sample_batch = inputs[0].example_tensor\n",
    "    sample_batch = get_sample_batch()\n",
    "    print('Sample input shape:', sample_batch.shape)\n",
    "    with torch.inference_mode():\n",
    "        prob = model(sample_batch)\n",
    "        print(prob.shape, prob)\n",
    "\n",
    "    # CONVERT TO TENSORRT\n",
    "    with torch.inference_mode():\n",
    "        print('START CONVERT')\n",
    "        trt_model = torch_tensorrt.compile(model,\n",
    "                                            inputs = inputs,\n",
    "                                            enabled_precisions = enable_precisions, # Run with FP32\n",
    "                                            workspace_size = 1 << 32,\n",
    "                                            )\n",
    "    torch.jit.save(trt_model, TRT_SAVE_PATH)\n",
    "    print('ALL DONE!')\n",
    "    \n",
    "\n",
    "def convert():\n",
    "    print(f'USING BACKEND {TRT_BACKEND}')\n",
    "    if TRT_BACKEND == 'torch_tensorrt':\n",
    "        convert_with_torch_tensorrt()\n",
    "    elif TRT_BACKEND == 'torch2trt':\n",
    "        convert_with_torch2trt()\n",
    "    else:\n",
    "        raise Exception()\n",
    "    \n",
    "    \n",
    "\n",
    "def test():\n",
    "    model_torch = KFoldEnsembleModel(MODEL_NAME, NUM_CLASSES, IN_CHANS, MODEL_CKPTS)\n",
    "    model_torch.eval()\n",
    "    model_torch.cuda()\n",
    "\n",
    "    if TRT_BACKEND == 'torch2trt':\n",
    "        model_trt = TRTModule()\n",
    "        model_trt.load_state_dict(torch.load(TRT_SAVE_PATH))\n",
    "    elif TRT_BACKEND == 'torch_tensorrt':\n",
    "        model_trt = torch.jit.load(TRT_SAVE_PATH)\n",
    "    else:\n",
    "        raise Exception()\n",
    "\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm(range(100)):\n",
    "            x = (torch.rand(BATCH_SIZE, 3, 2048, 1024).float() * 255).cuda()\n",
    "            prob_trt = model_trt(x)\n",
    "            prob_torch = model_torch(x)\n",
    "            print(prob_torch.shape, prob_trt.shape)\n",
    "            diff = torch.max(torch.abs(prob_torch - prob_trt))\n",
    "#             print(prob_torch)\n",
    "#             print(prob_trt)\n",
    "#             print(prob_torch.shape, prob_trt.shape, diff)\n",
    "            if diff > 1e-5:\n",
    "                print(diff)\n",
    "            assert diff < 1e-4, str(diff)\n",
    "            \n",
    "\n",
    "def speed_torch(num = 100, amp = False):\n",
    "    model_torch = KFoldEnsembleModel(MODEL_NAME, NUM_CLASSES, IN_CHANS, MODEL_CKPTS)\n",
    "    model_torch.eval()\n",
    "    model_torch.cuda()\n",
    "    \n",
    "    if amp:\n",
    "        from functools import partial\n",
    "        AMP_DTYPE = torch.float16\n",
    "        # AMP_DTYPE = torch.bfloat16\n",
    "        amp_autocast = partial(torch.autocast, device_type='cuda', dtype=AMP_DTYPE)\n",
    "\n",
    "    x = (torch.rand(BATCH_SIZE, 3, 2048, 1024).float() * 255).cuda()\n",
    "    torch.cuda.synchronize()\n",
    "    takes = []\n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm(range(num)):\n",
    "            print(x.shape)\n",
    "            start = time.time()\n",
    "            if amp:\n",
    "                with amp_autocast():\n",
    "                    prob_torch = model_torch(x)\n",
    "            else:\n",
    "                prob_torch = model_torch(x)\n",
    "#             print(prob_torch.shape)\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            take = end - start\n",
    "            takes.append(take)\n",
    "            print(f'{prob_torch.shape} take {round(take * 1000, 2)} ms')\n",
    "    takes = np.array(takes)\n",
    "    print(f'Mean: {takes.mean()}, std: {takes.std()}')\n",
    "    print('Time per sample:', takes.sum() / (num * BATCH_SIZE))\n",
    "    print('Throughput:', (num * BATCH_SIZE) / takes.sum())\n",
    "    \n",
    "\n",
    "def speed_trt(num = 100):\n",
    "    if TRT_BACKEND == 'torch2trt':\n",
    "        model_trt = TRTModule()\n",
    "        model_trt.load_state_dict(torch.load(TRT_SAVE_PATH))\n",
    "    elif TRT_BACKEND == 'torch_tensorrt':\n",
    "        model_trt = torch.jit.load(TRT_SAVE_PATH)\n",
    "    else:\n",
    "        raise Exception()\n",
    "\n",
    "    x = (torch.rand(1, 3, 2048, 1024).float() * 255).cuda()\n",
    "    torch.cuda.synchronize()\n",
    "    takes = []\n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm(range(num)):\n",
    "            start = time.time()\n",
    "            prob_trt = model_trt(x)\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            take = end - start\n",
    "            takes.append(take)\n",
    "            print(f'{prob_trt.shape}, take {round(take * 1000, 2)} ms')\n",
    "    takes = np.array(takes)\n",
    "    print(f'Mean: {takes.mean()}, std: {takes.std()}')\n",
    "    print('Time per sample:', takes.sum() / (num * BATCH_SIZE))\n",
    "    print('Throughput:', (num * BATCH_SIZE) / takes.sum())\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6419badf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:14:14.969286Z",
     "iopub.status.busy": "2023-02-27T15:14:14.968526Z",
     "iopub.status.idle": "2023-02-27T15:14:14.972821Z",
     "shell.execute_reply": "2023-02-27T15:14:14.971924Z"
    },
    "papermill": {
     "duration": 0.018577,
     "end_time": "2023-02-27T15:14:14.974848",
     "exception": false,
     "start_time": "2023-02-27T15:14:14.956271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import convert_model\n",
    "# importlib.reload(convert_model)\n",
    "# import convert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e6ea3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:14:14.998082Z",
     "iopub.status.busy": "2023-02-27T15:14:14.997802Z",
     "iopub.status.idle": "2023-02-27T15:14:15.002513Z",
     "shell.execute_reply": "2023-02-27T15:14:15.001626Z"
    },
    "papermill": {
     "duration": 0.018707,
     "end_time": "2023-02-27T15:14:15.004457",
     "exception": false,
     "start_time": "2023-02-27T15:14:14.985750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert_model.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bbdba8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:14:15.028400Z",
     "iopub.status.busy": "2023-02-27T15:14:15.027615Z",
     "iopub.status.idle": "2023-02-27T15:14:15.032057Z",
     "shell.execute_reply": "2023-02-27T15:14:15.031080Z"
    },
    "papermill": {
     "duration": 0.018384,
     "end_time": "2023-02-27T15:14:15.033972",
     "exception": false,
     "start_time": "2023-02-27T15:14:15.015588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f1cbbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:14:15.057993Z",
     "iopub.status.busy": "2023-02-27T15:14:15.057217Z",
     "iopub.status.idle": "2023-02-27T15:14:15.061498Z",
     "shell.execute_reply": "2023-02-27T15:14:15.060622Z"
    },
    "papermill": {
     "duration": 0.018271,
     "end_time": "2023-02-27T15:14:15.063445",
     "exception": false,
     "start_time": "2023-02-27T15:14:15.045174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert_model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e149d92f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:14:15.088479Z",
     "iopub.status.busy": "2023-02-27T15:14:15.087747Z",
     "iopub.status.idle": "2023-02-27T15:14:15.091800Z",
     "shell.execute_reply": "2023-02-27T15:14:15.090644Z"
    },
    "papermill": {
     "duration": 0.019037,
     "end_time": "2023-02-27T15:14:15.093839",
     "exception": false,
     "start_time": "2023-02-27T15:14:15.074802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3854f135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:14:15.118027Z",
     "iopub.status.busy": "2023-02-27T15:14:15.117143Z",
     "iopub.status.idle": "2023-02-27T15:14:15.121198Z",
     "shell.execute_reply": "2023-02-27T15:14:15.120344Z"
    },
    "papermill": {
     "duration": 0.018119,
     "end_time": "2023-02-27T15:14:15.123103",
     "exception": false,
     "start_time": "2023-02-27T15:14:15.104984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch_tensorrt\n",
    "# torch_tensorrt.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8675bfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:14:15.147813Z",
     "iopub.status.busy": "2023-02-27T15:14:15.147014Z",
     "iopub.status.idle": "2023-02-27T15:14:15.151536Z",
     "shell.execute_reply": "2023-02-27T15:14:15.150530Z"
    },
    "papermill": {
     "duration": 0.018898,
     "end_time": "2023-02-27T15:14:15.153531",
     "exception": false,
     "start_time": "2023-02-27T15:14:15.134633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert_model.speed_torch(10, amp = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d937713",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:14:15.178153Z",
     "iopub.status.busy": "2023-02-27T15:14:15.177862Z",
     "iopub.status.idle": "2023-02-27T15:14:15.181677Z",
     "shell.execute_reply": "2023-02-27T15:14:15.180675Z"
    },
    "papermill": {
     "duration": 0.018662,
     "end_time": "2023-02-27T15:14:15.183641",
     "exception": false,
     "start_time": "2023-02-27T15:14:15.164979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert_model.speed_trt(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f1be3",
   "metadata": {
    "papermill": {
     "duration": 0.011212,
     "end_time": "2023-02-27T15:14:15.206247",
     "exception": false,
     "start_time": "2023-02-27T15:14:15.195035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05faca0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:14:15.231199Z",
     "iopub.status.busy": "2023-02-27T15:14:15.230853Z",
     "iopub.status.idle": "2023-02-27T15:15:00.287058Z",
     "shell.execute_reply": "2023-02-27T15:15:00.285448Z"
    },
    "papermill": {
     "duration": 45.071522,
     "end_time": "2023-02-27T15:15:00.289320",
     "exception": false,
     "start_time": "2023-02-27T15:14:15.217798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATIENT CHUNKS: [1, 0]\n",
      "Processing chunk 0 with 1 patients, 4 images\n",
      "Starting 3 jobs with backend `joblib`, 3 chunks ...\n",
      "[02/27/2023-15:14:26] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.3.2\n",
      "[02/27/2023-15:14:26] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.3.2\n",
      "[02/27/2023-15:14:27] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.3.2\n",
      "[02/27/2023-15:14:27] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.3.2\n",
      "[02/27/2023-15:14:27] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.3.2\n",
      "[02/27/2023-15:14:27] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.3.2\n",
      "ROI extractor (YOLOX) loaded!\n",
      "ROI extractor (YOLOX) loaded!\n",
      "ROI extractor (YOLOX) loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:08<00:00,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DALI done in 18.79310917854309 sec\n",
      "Number of undecoded files: 0\n",
      "No remain files to decode.\n",
      "SDL done in 0.0005154609680175781 sec\n",
      "TOTAL DECODING TIME: 18.79362463951111 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:00<00:00, 7752.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading dataset with 4 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/27/2023-15:14:55] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.3.2\n",
      "[02/27/2023-15:14:55] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:04<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE CHUNK 0 with 4 samples\n",
      "Removed save image directory /kaggle/tmp/pngs\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Processing chunk 1 with 0 patients, 0 images\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "\n",
    "from metrics import compute_metrics\n",
    "\n",
    "os.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n",
    "import ctypes\n",
    "import gc\n",
    "import importlib\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import dicomsdl\n",
    "import numpy as np\n",
    "import nvidia.dali as dali\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import roi_extract\n",
    "import torch\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from joblib import Parallel, delayed\n",
    "from nvidia.dali import types\n",
    "from nvidia.dali.backend import TensorGPU, TensorListGPU\n",
    "from torch2trt import TRTModule\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "importlib.reload(roi_extract)\n",
    "import roi_extract\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "THRES = 0.31\n",
    "AUTO_THRES = False\n",
    "AUTO_THRES_PERCENTILE = 0.97935\n",
    "\n",
    "J2K_SUID = '1.2.840.10008.1.2.4.90'\n",
    "J2K_HEADER = b\"\\x00\\x00\\x00\\x0C\"\n",
    "JLL_SUID = '1.2.840.10008.1.2.4.70'\n",
    "JLL_HEADER = b\"\\xff\\xd8\\xff\\xe0\"\n",
    "SUID2HEADER = {J2K_SUID: J2K_HEADER, JLL_SUID: JLL_HEADER}\n",
    "VOILUT_FUNCS_MAP = {'LINEAR': 0, 'LINEAR_EXACT': 1, 'SIGMOID': 2}\n",
    "VOILUT_FUNCS_INV_MAP = {v: k for k, v in VOILUT_FUNCS_MAP.items()}\n",
    "# roi detection\n",
    "ROI_YOLOX_INPUT_SIZE = [416, 416]\n",
    "ROI_YOLOX_CONF_THRES = 0.5\n",
    "ROI_YOLOX_NMS_THRES = 0.9\n",
    "ROI_YOLOX_HW = [(52, 52), (26, 26), (13, 13)]\n",
    "ROI_YOLOX_STRIDES = [8, 16, 32]\n",
    "ROI_AREA_PCT_THRES = 0.04\n",
    "# model\n",
    "MODEL_INPUT_SIZE = [2048, 1024]\n",
    "\n",
    "MODE = 'KAGGLE-TEST'\n",
    "assert MODE in ['LOCAL-VAL', 'KAGGLE-VAL', 'KAGGLE-TEST']\n",
    "\n",
    "if MODE == 'KAGGLE-VAL':\n",
    "    ROI_YOLOX_ENGINE_PATH = '/kaggle/input/kaggle-rsna-ckpts/yolox_nano_bre_416_v2_trt.pth'\n",
    "    CSV_PATH = '/kaggle/input/kaggle-rsna-ckpts/val_fold_0.csv'\n",
    "    DCM_ROOT_DIR = '/kaggle/input/rsna-breast-cancer-detection/train_images'\n",
    "    SAVE_IMG_ROOT_DIR = '/kaggle/tmp/pngs'\n",
    "    N_CHUNKS = 2\n",
    "    N_CPUS = 2\n",
    "    RM_DONE_CHUNK = False\n",
    "elif MODE == 'KAGGLE-TEST':\n",
    "    ROI_YOLOX_ENGINE_PATH = '/kaggle/input/kaggle-rsna-ckpts/yolox_nano_bre_416_v2_trt.pth'\n",
    "    CSV_PATH = '/kaggle/input/rsna-breast-cancer-detection/test.csv'\n",
    "    DCM_ROOT_DIR = '/kaggle/input/rsna-breast-cancer-detection/test_images'\n",
    "    SAVE_IMG_ROOT_DIR = '/kaggle/tmp/pngs'\n",
    "    N_CHUNKS = 2\n",
    "    N_CPUS = 2\n",
    "    RM_DONE_CHUNK = True\n",
    "elif MODE == 'LOCAL-VAL':\n",
    "    ROI_YOLOX_ENGINE_PATH = '../roi_det/YOLOX/YOLOX_outputs/yolox_nano_bre_416/model_trt.pth'\n",
    "    CSV_PATH = '../../datasets/cv/v1/val_fold_0.csv'\n",
    "    DCM_ROOT_DIR = '../../datasets/train_images/'\n",
    "    SAVE_IMG_ROOT_DIR = './temp_save'\n",
    "    N_CHUNKS = 2\n",
    "    N_CPUS = 2\n",
    "    RM_DONE_CHUNK = False\n",
    "\n",
    "# DALI patch for INT16 support\n",
    "################################################################################\n",
    "DALI2TORCH_TYPES = {\n",
    "    types.DALIDataType.FLOAT: torch.float32,\n",
    "    types.DALIDataType.FLOAT64: torch.float64,\n",
    "    types.DALIDataType.FLOAT16: torch.float16,\n",
    "    types.DALIDataType.UINT8: torch.uint8,\n",
    "    types.DALIDataType.INT8: torch.int8,\n",
    "    types.DALIDataType.UINT16: torch.int16,\n",
    "    types.DALIDataType.INT16: torch.int16,\n",
    "    types.DALIDataType.INT32: torch.int32,\n",
    "    types.DALIDataType.INT64: torch.int64\n",
    "}\n",
    "\n",
    "\n",
    "# @TODO: dangerous to copy from UINT16 to INT16 (memory layout?)\n",
    "# little/big endian ?\n",
    "# @TODO: faster reuse memory without copying: https://github.com/NVIDIA/DALI/issues/4126\n",
    "def feed_ndarray(dali_tensor, arr, cuda_stream=None):\n",
    "    \"\"\"\n",
    "    Copy contents of DALI tensor to PyTorch's Tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    `dali_tensor` : nvidia.dali.backend.TensorCPU or nvidia.dali.backend.TensorGPU\n",
    "                    Tensor from which to copy\n",
    "    `arr` : torch.Tensor\n",
    "            Destination of the copy\n",
    "    `cuda_stream` : torch.cuda.Stream, cudaStream_t or any value that can be cast to cudaStream_t.\n",
    "                    CUDA stream to be used for the copy\n",
    "                    (if not provided, an internal user stream will be selected)\n",
    "                    In most cases, using pytorch's current stream is expected (for example,\n",
    "                    if we are copying to a tensor allocated with torch.zeros(...))\n",
    "    \"\"\"\n",
    "    dali_type = DALI2TORCH_TYPES[dali_tensor.dtype]\n",
    "\n",
    "    assert dali_type == arr.dtype, (\n",
    "        \"The element type of DALI Tensor/TensorList\"\n",
    "        \" doesn't match the element type of the target PyTorch Tensor: \"\n",
    "        \"{} vs {}\".format(dali_type, arr.dtype))\n",
    "    assert dali_tensor.shape() == list(arr.size()), \\\n",
    "        (\"Shapes do not match: DALI tensor has size {0}, but PyTorch Tensor has size {1}\".\n",
    "            format(dali_tensor.shape(), list(arr.size())))\n",
    "    cuda_stream = types._raw_cuda_stream(cuda_stream)\n",
    "\n",
    "    # turn raw int to a c void pointer\n",
    "    c_type_pointer = ctypes.c_void_p(arr.data_ptr())\n",
    "    if isinstance(dali_tensor, (TensorGPU, TensorListGPU)):\n",
    "        stream = None if cuda_stream is None else ctypes.c_void_p(cuda_stream)\n",
    "        dali_tensor.copy_to_external(c_type_pointer, stream, non_blocking=True)\n",
    "    else:\n",
    "        dali_tensor.copy_to_external(c_type_pointer)\n",
    "    return arr\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class PydicomMetadata:\n",
    "\n",
    "    def __init__(self, ds):\n",
    "        if \"WindowWidth\" not in ds or \"WindowCenter\" not in ds:\n",
    "            self.window_widths = []\n",
    "            self.window_centers = []\n",
    "        else:\n",
    "            ww = ds['WindowWidth']\n",
    "            wc = ds['WindowCenter']\n",
    "            self.window_widths = [float(e)\n",
    "                                  for e in ww] if ww.VM > 1 else [float(ww.value)]\n",
    "\n",
    "            self.window_centers = [float(e) for e in wc\n",
    "                                   ] if wc.VM > 1 else [float(wc.value)]\n",
    "        \n",
    "        # if nan --> LINEAR\n",
    "        self.voilut_func = str(ds.get('VOILUTFunction', 'LINEAR')).upper()\n",
    "        self.invert = (ds.PhotometricInterpretation == 'MONOCHROME1')\n",
    "        assert len(self.window_widths) == len(self.window_centers)\n",
    "\n",
    "\n",
    "class DicomsdlMetadata:\n",
    "\n",
    "    def __init__(self, ds):\n",
    "        self.window_widths = ds.WindowWidth\n",
    "        self.window_centers = ds.WindowCenter\n",
    "        if self.window_widths is None or self.window_centers is None:\n",
    "            self.window_widths = []\n",
    "            self.window_centers = []\n",
    "        else:\n",
    "            try:\n",
    "                if not isinstance(self.window_widths, list):\n",
    "                    self.window_widths = [self.window_widths]\n",
    "                self.window_widths = [float(e) for e in self.window_widths]\n",
    "                if not isinstance(self.window_centers, list):\n",
    "                    self.window_centers = [self.window_centers]\n",
    "                self.window_centers = [float(e) for e in self.window_centers]\n",
    "            except:\n",
    "                self.window_widths = []\n",
    "                self.window_centers = []\n",
    "                \n",
    "        # if nan --> LINEAR\n",
    "        self.voilut_func = ds.VOILUTFunction\n",
    "        if self.voilut_func is None:\n",
    "            self.voilut_func = 'LINEAR'\n",
    "        else:\n",
    "            self.voilut_func = str(self.voilut_func).upper()\n",
    "        self.invert = (ds.PhotometricInterpretation == 'MONOCHROME1')\n",
    "        assert len(self.window_widths) == len(self.window_centers)\n",
    "\n",
    "\n",
    "def min_max_scale(img):\n",
    "    maxv = img.max()\n",
    "    minv = img.min()\n",
    "    if maxv > minv:\n",
    "        return (img - minv) / (maxv - minv)\n",
    "    else:\n",
    "        return img - minv  # ==0\n",
    "\n",
    "\n",
    "# DEPRECATED: too slow :D\n",
    "# from Pydicom's source\n",
    "def apply_windowing_np(arr,\n",
    "                       window_width=None,\n",
    "                       window_center=None,\n",
    "                       voi_func='LINEAR',\n",
    "                       y_min=0,\n",
    "                       y_max=255):\n",
    "    print('WARNING: Deprecated. Using apply_windowing_np_v2() instead.')\n",
    "    assert window_width > 0\n",
    "    y_range = y_max - y_min\n",
    "    # float64 needed (default) or just float32 ?\n",
    "    # arr = arr.astype(np.float64)\n",
    "    arr = arr.astype(np.float32)\n",
    "\n",
    "    if voi_func in ['LINEAR', 'LINEAR_EXACT']:\n",
    "        # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n",
    "        if voi_func == 'LINEAR':\n",
    "            if window_width < 1:\n",
    "                raise ValueError(\n",
    "                    \"The (0028,1051) Window Width must be greater than or \"\n",
    "                    \"equal to 1 for a 'LINEAR' windowing operation\")\n",
    "            window_center -= 0.5\n",
    "            window_width -= 1\n",
    "        below = arr <= (window_center - window_width / 2)\n",
    "        above = arr > (window_center + window_width / 2)\n",
    "        between = np.logical_and(~below, ~above)\n",
    "\n",
    "        arr[below] = y_min\n",
    "        arr[above] = y_max\n",
    "        if between.any():\n",
    "            arr[between] = ((\n",
    "                (arr[between] - window_center) / window_width + 0.5) * y_range\n",
    "                            + y_min)\n",
    "    elif voi_func == 'SIGMOID':\n",
    "        arr = y_range / (1 +\n",
    "                         np.exp(-4 *\n",
    "                                (arr - window_center) / window_width)) + y_min\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "def apply_windowing_np_v2(arr,\n",
    "                          window_width=None,\n",
    "                          window_center=None,\n",
    "                          voi_func='LINEAR',\n",
    "                          y_min=0,\n",
    "                          y_max=255):\n",
    "    assert window_width > 0\n",
    "    y_range = y_max - y_min\n",
    "    # float64 needed (default) or just float32 ?\n",
    "    # arr = arr.astype(np.float64)\n",
    "    arr = arr.astype(np.float32)\n",
    "\n",
    "    if voi_func == 'LINEAR' or voi_func == 'LINEAR_EXACT':\n",
    "        # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n",
    "        if voi_func == 'LINEAR':\n",
    "            if window_width < 1:\n",
    "                raise ValueError(\n",
    "                    \"The (0028,1051) Window Width must be greater than or \"\n",
    "                    \"equal to 1 for a 'LINEAR' windowing operation\")\n",
    "            window_center -= 0.5\n",
    "            window_width -= 1\n",
    "\n",
    "        # simple trick to improve speed\n",
    "        s = y_range / window_width\n",
    "        b = (-window_center / window_width + 0.5) * y_range + y_min\n",
    "        arr = arr * s + b\n",
    "        arr = np.clip(arr, y_min, y_max)\n",
    "\n",
    "    elif voi_func == 'SIGMOID':\n",
    "        # simple trick to improve speed\n",
    "        s = -4 / window_width\n",
    "        arr = y_range / (1 + np.exp((arr - window_center) * s)) + y_min\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "def apply_windowing_torch(arr,\n",
    "                          window_width=None,\n",
    "                          window_center=None,\n",
    "                          voi_func='LINEAR',\n",
    "                          y_min=0,\n",
    "                          y_max=255):\n",
    "    assert window_width > 0\n",
    "    y_range = y_max - y_min\n",
    "    # float64 needed (default) or just float32 ?\n",
    "    # arr = arr.double()\n",
    "    arr = arr.float()\n",
    "\n",
    "    if voi_func == 'LINEAR' or voi_func == 'LINEAR_EXACT':\n",
    "        # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n",
    "        if voi_func == 'LINEAR':\n",
    "            if window_width < 1:\n",
    "                raise ValueError(\n",
    "                    \"The (0028,1051) Window Width must be greater than or \"\n",
    "                    \"equal to 1 for a 'LINEAR' windowing operation\")\n",
    "            window_center -= 0.5\n",
    "            window_width -= 1\n",
    "\n",
    "        # simple trick to improve speed\n",
    "        s = y_range / window_width\n",
    "        b = (-window_center / window_width + 0.5) * y_range + y_min\n",
    "        arr = arr * s + b\n",
    "        arr = torch.clamp(arr, y_min, y_max)\n",
    "\n",
    "    elif voi_func == 'SIGMOID':\n",
    "        # simple trick to improve speed\n",
    "        s = -4 / window_width\n",
    "        arr = y_range / (1 + torch.exp((arr - window_center) * s)) + y_min\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "def resize_and_pad(img, input_size=MODEL_INPUT_SIZE):\n",
    "    input_h, input_w = input_size\n",
    "    ori_h, ori_w = img.shape[:2]\n",
    "    ratio = min(input_h / ori_h, input_w / ori_w)\n",
    "    # resize\n",
    "    img = F.interpolate(img.view(1, 1, ori_h, ori_w),\n",
    "                        mode=\"bilinear\",\n",
    "                        scale_factor=ratio,\n",
    "                        recompute_scale_factor=True)[0, 0]\n",
    "    # padding\n",
    "    padded_img = torch.zeros((input_h, input_w),\n",
    "                             dtype=img.dtype,\n",
    "                             device='cuda')\n",
    "    cur_h, cur_w = img.shape\n",
    "    y_start = (input_h - cur_h) // 2\n",
    "    x_start = (input_w - cur_w) // 2\n",
    "    padded_img[y_start:y_start + cur_h, x_start: x_start + cur_w] = img\n",
    "    padded_img = padded_img.unsqueeze(-1).expand(-1, -1, 3)\n",
    "    return padded_img\n",
    "\n",
    "\n",
    "def save_img_to_file(save_path, img, backend='cv2'):\n",
    "    file_ext = os.path.basename(save_path).split('.')[-1]\n",
    "    if backend == 'cv2':\n",
    "        if img.dtype == np.uint16:\n",
    "            # https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce\n",
    "            assert file_ext in ['png', 'jp2', 'tiff', 'tif']\n",
    "            cv2.imwrite(save_path, img)\n",
    "        elif img.dtype == np.uint8:\n",
    "            cv2.imwrite(save_path, img)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                '`cv2` backend only support uint8 or uint16 images.')\n",
    "    elif backend == 'np':\n",
    "        assert file_ext == 'npy'\n",
    "        np.save(save_path, img)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported backend `{backend}`.')\n",
    "\n",
    "\n",
    "def load_img_from_file(img_path, backend='cv2'):\n",
    "    if backend == 'cv2':\n",
    "        return cv2.imread(img_path, cv2.IMREAD_ANYDEPTH)\n",
    "    elif backend == 'np':\n",
    "        return np.load(img_path)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "\n",
    "class _JStreamExternalSource:\n",
    "\n",
    "    def __init__(self, dcm_paths, batch_size=1):\n",
    "        self.dcm_paths = dcm_paths\n",
    "        self.len = len(dcm_paths)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __call__(self, batch_info):\n",
    "        idx = batch_info.iteration\n",
    "        # print('IDX:', batch_info.iteration, batch_info.epoch_idx)\n",
    "        start = idx * self.batch_size\n",
    "        end = min(self.len, start + self.batch_size)\n",
    "        if end <= start:\n",
    "            raise StopIteration()\n",
    "\n",
    "        batch_dcm_paths = self.dcm_paths[start:end]\n",
    "        j_streams = []\n",
    "        inverts = []\n",
    "        windowing_params = []\n",
    "        voilut_funcs = []\n",
    "\n",
    "        for dcm_path in batch_dcm_paths:\n",
    "            ds = pydicom.dcmread(dcm_path)\n",
    "            pixel_data = ds.PixelData\n",
    "            offset = pixel_data.find(\n",
    "                SUID2HEADER[ds.file_meta.TransferSyntaxUID])\n",
    "            \n",
    "#             for ss in [10152, 10315, 10342, 1036, 10432, 10439, 12678]:\n",
    "#                 if str(ss) in dcm_path:\n",
    "#                     offset += 2\n",
    "#             if idx % 5 == 4:\n",
    "#                 offset += 2\n",
    "                    \n",
    "            j_stream = np.array(bytearray(pixel_data[offset:]), np.uint8)\n",
    "            invert = (ds.PhotometricInterpretation == 'MONOCHROME1')\n",
    "            meta = PydicomMetadata(ds)\n",
    "            windowing_param = np.array(\n",
    "                [meta.window_centers, meta.window_widths], np.float16)\n",
    "            voilut_func = VOILUT_FUNCS_MAP[meta.voilut_func]\n",
    "            j_streams.append(j_stream)\n",
    "            inverts.append(invert)\n",
    "            windowing_params.append(windowing_param)\n",
    "            voilut_funcs.append(voilut_func)\n",
    "#         return j_streams, inverts, windowing_params, voilut_funcs\n",
    "        return j_streams, np.array(inverts, dtype=np.bool_), \\\n",
    "            windowing_params, np.array(voilut_funcs, dtype=np.uint8)\n",
    "\n",
    "\n",
    "@dali.pipeline_def\n",
    "def _dali_pipeline(eii):\n",
    "    jpeg, invert, windowing_param, voilut_func = dali.fn.external_source(\n",
    "        source=eii,\n",
    "        num_outputs=4,\n",
    "        dtype=[\n",
    "            dali.types.UINT8, dali.types.BOOL, dali.types.FLOAT16,\n",
    "            dali.types.UINT8\n",
    "        ],\n",
    "        batch=True,\n",
    "        batch_info=True,\n",
    "        parallel=True)\n",
    "    ori_img = dali.fn.experimental.decoders.image(\n",
    "        jpeg,\n",
    "        device='mixed',\n",
    "        output_type=dali.types.ANY_DATA,\n",
    "        dtype=dali.types.UINT16)\n",
    "    return ori_img, invert, windowing_param, voilut_func\n",
    "\n",
    "\n",
    "def decode_crop_save_dali(dcm_paths,\n",
    "                          save_paths,\n",
    "                          save_backend='cv2',\n",
    "                          batch_size=1,\n",
    "                          num_threads=1,\n",
    "                          py_num_workers=1,\n",
    "                          py_start_method='fork',\n",
    "                          device_id=0):\n",
    "\n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "    assert save_backend in ['cv2', 'np']\n",
    "    num_dcms = len(dcm_paths)\n",
    "\n",
    "    # dali to process with chunk in-memory\n",
    "    external_source = _JStreamExternalSource(dcm_paths, batch_size=batch_size)\n",
    "    pipe = _dali_pipeline(\n",
    "        external_source,\n",
    "        py_num_workers=py_num_workers,\n",
    "        py_start_method=py_start_method,\n",
    "        batch_size=batch_size,\n",
    "        num_threads=num_threads,\n",
    "        device_id=device_id,\n",
    "        debug=False,\n",
    "    )\n",
    "    pipe.build()\n",
    "\n",
    "    roi_extractor = roi_extract.RoiExtractor(engine_path=ROI_YOLOX_ENGINE_PATH,\n",
    "                                             input_size=ROI_YOLOX_INPUT_SIZE,\n",
    "                                             num_classes=1,\n",
    "                                             conf_thres=ROI_YOLOX_CONF_THRES,\n",
    "                                             nms_thres=ROI_YOLOX_NMS_THRES,\n",
    "                                             class_agnostic=False,\n",
    "                                             area_pct_thres=ROI_AREA_PCT_THRES,\n",
    "                                             hw=ROI_YOLOX_HW,\n",
    "                                             strides=ROI_YOLOX_STRIDES,\n",
    "                                             exp=None)\n",
    "    print('ROI extractor (YOLOX) loaded!')\n",
    "\n",
    "    num_batchs = num_dcms // batch_size\n",
    "    last_batch_size = batch_size\n",
    "    if num_dcms % batch_size > 0:\n",
    "        num_batchs += 1\n",
    "        last_batch_size = num_dcms % batch_size\n",
    "\n",
    "    cur_idx = -1\n",
    "    for _batch_idx in tqdm(range(num_batchs)):\n",
    "        try:\n",
    "            outs = pipe.run()\n",
    "        except Exception as e:\n",
    "#             print('DALI exception occur:', e)\n",
    "            print(\n",
    "                f'Exception: One of {dcm_paths[_batch_idx * batch_size: (_batch_idx + 1) * batch_size]} can not be decoded.'\n",
    "            )\n",
    "            # ignore this batch and re-build pipeline\n",
    "            if _batch_idx < num_batchs - 1:\n",
    "                cur_idx += batch_size\n",
    "                del external_source, pipe\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                external_source = _JStreamExternalSource(\n",
    "                    dcm_paths[(_batch_idx + 1) * batch_size:],\n",
    "                    batch_size=batch_size)\n",
    "                pipe = _dali_pipeline(\n",
    "                    external_source,\n",
    "                    py_num_workers=py_num_workers,\n",
    "                    py_start_method=py_start_method,\n",
    "                    batch_size=batch_size,\n",
    "                    num_threads=num_threads,\n",
    "                    device_id=device_id,\n",
    "                    debug=False,\n",
    "                )\n",
    "                pipe.build()\n",
    "            else:\n",
    "                cur_idx += last_batch_size\n",
    "            continue\n",
    "\n",
    "        imgs = outs[0]\n",
    "        inverts = outs[1]\n",
    "        windowing_params = outs[2]\n",
    "        voilut_funcs = outs[3]\n",
    "        for j in range(len(inverts)):\n",
    "            cur_idx += 1\n",
    "            save_path = save_paths[cur_idx]\n",
    "            img_dali = imgs[j]\n",
    "            img_torch = torch.empty(img_dali.shape(),\n",
    "                                    dtype=torch.int16,\n",
    "                                    device='cuda')\n",
    "            feed_ndarray(img_dali,\n",
    "                         img_torch,\n",
    "                         cuda_stream=torch.cuda.current_stream(device=0))\n",
    "            # @TODO: test whether copy uint16 to int16 pointer is safe in this case\n",
    "            if 0:\n",
    "                img_np = img_dali.as_cpu().squeeze(-1)  # uint16\n",
    "                print(type(img_np), img_np.shape)\n",
    "                img_np = torch.from_numpy(img_np, dtype=torch.int16)\n",
    "                diff = torch.max(torch.abs(img_np - img_torch))\n",
    "                assert diff == 0, f'{img_torch.shape}, {img_np.shape}, {diff}'\n",
    "\n",
    "            invert = inverts.at(j).item()\n",
    "            windowing_param = windowing_params.at(j)\n",
    "            voilut_func = voilut_funcs.at(j).item()\n",
    "            voilut_func = VOILUT_FUNCS_INV_MAP[voilut_func]\n",
    "\n",
    "            # YOLOX for ROI extraction\n",
    "            # @TODO: optimize and sync training vs inference pipeline:\n",
    "            # @current: uint16 --> min_max_norm --> uint8 --> resize (cv2) --> padding\n",
    "            # @faster?: uint16 --> resize (torch interpolate) --> min_max_norm --> uint8 --> padding\n",
    "            # @fastest?: uint16 --> VOILUT --> resize (torch interpolate) --> uint8 --> padding\n",
    "            img_yolox = min_max_scale(img_torch)\n",
    "            img_yolox = (img_yolox * 255)  # float32\n",
    "            if invert:\n",
    "                img_yolox = 255 - img_yolox\n",
    "            # YOLOX infer\n",
    "            # who know if exception happen in hidden test ?\n",
    "            try:\n",
    "                xyxy, _area_pct, _conf = roi_extractor.detect_single(img_yolox)\n",
    "                if xyxy is not None:\n",
    "                    x0, y0, x1, y1 = xyxy\n",
    "                    crop = img_torch[y0:y1, x0:x1]\n",
    "                else:\n",
    "                    crop = img_torch\n",
    "            except:\n",
    "                print('ROI extract exception!')\n",
    "                crop = img_torch\n",
    "\n",
    "            # apply windowing\n",
    "            if windowing_param.shape[1] != 0:\n",
    "                default_window_center = windowing_param[0, 0]\n",
    "                default_window_width = windowing_param[1, 0]\n",
    "                crop = apply_windowing_torch(crop,\n",
    "                                             window_width=default_window_width,\n",
    "                                             window_center=default_window_center,\n",
    "                                             voi_func=voilut_func,\n",
    "                                             y_min=0,\n",
    "                                             y_max=255)\n",
    "            # if no window center/width in dcm file\n",
    "            # do simple min-max scaling\n",
    "            else:\n",
    "                print('No windowing param!')\n",
    "                crop = min_max_scale(crop)\n",
    "                crop = crop * 255\n",
    "            if invert:\n",
    "                crop = 255 - crop\n",
    "            crop = resize_and_pad(crop, MODEL_INPUT_SIZE)\n",
    "            crop = crop.to(torch.uint8)\n",
    "            crop = crop.cpu().numpy()\n",
    "            save_img_to_file(save_path, crop, backend=save_backend)\n",
    "\n",
    "#     assert cur_idx == len(\n",
    "#         save_paths) - 1, f'{cur_idx} != {len(save_paths) - 1}'\n",
    "\n",
    "    try:\n",
    "        del external_source, pipe, roi_extractor\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return\n",
    "\n",
    "\n",
    "def decode_and_save_dali_parallel(\n",
    "        dcm_paths,\n",
    "        save_paths,\n",
    "        save_backend='cv2',\n",
    "        batch_size=1,\n",
    "        num_threads=1,\n",
    "        py_num_workers=1,\n",
    "        py_start_method='fork',\n",
    "        device_id=0,\n",
    "        parallel_n_jobs=2,\n",
    "        parallel_n_chunks = 4,\n",
    "        parallel_backend='joblib',  # joblib or multiprocessing\n",
    "        joblib_backend='loky'):\n",
    "    assert parallel_backend in ['joblib', 'multiprocessing']\n",
    "    assert joblib_backend in ['threading', 'multiprocessing', 'loky']\n",
    "    # py_num_workers > 0 means using multiprocessing worker\n",
    "    # 'fork' multiprocessing after CUDA init is not work (we must use 'spawn' instead)\n",
    "    # since our pipeline can be re-build (when a dicom can't be decoded on GPU),\n",
    "    # 2 options:\n",
    "    #       (py_num_workers = 0, py_start_method=?)\n",
    "    #       (py_num_workers > 0, py_start_method = 'spawn')\n",
    "    assert not (py_num_workers > 0 and py_start_method == 'fork')\n",
    "\n",
    "    if parallel_n_jobs == 1:\n",
    "        print('No parralel. Starting the tasks within current process.')\n",
    "        return decode_crop_save_dali(dcm_paths,\n",
    "                                     save_paths,\n",
    "                                     save_backend=save_backend,\n",
    "                                     batch_size=batch_size,\n",
    "                                     num_threads=num_threads,\n",
    "                                     py_num_workers=py_num_workers,\n",
    "                                     py_start_method=py_start_method,\n",
    "                                     device_id=device_id)\n",
    "    else:\n",
    "        num_samples = len(dcm_paths)\n",
    "        num_samples_per_chunk = num_samples // parallel_n_chunks\n",
    "        if num_samples % parallel_n_chunks > 0:\n",
    "            num_samples_per_chunk += 1\n",
    "        starts = [num_samples_per_chunk * i for i in range(parallel_n_chunks)]\n",
    "        ends = [\n",
    "            min(start + num_samples_per_chunk, num_samples)\n",
    "            for start in starts\n",
    "        ]\n",
    "        if isinstance(device_id, list):\n",
    "            assert len(device_id) == parallel_n_chunks\n",
    "        elif isinstance(device_id, int):\n",
    "            device_id = [device_id] * parallel_n_chunks\n",
    "\n",
    "        print(\n",
    "            f'Starting {parallel_n_jobs} jobs with backend `{parallel_backend}`, {parallel_n_chunks} chunks ...'\n",
    "        )\n",
    "        if parallel_backend == 'joblib':\n",
    "            _ = Parallel(n_jobs=parallel_n_jobs, backend=joblib_backend)(\n",
    "                delayed(decode_crop_save_dali)(\n",
    "                    dcm_paths[start:end],\n",
    "                    save_paths[start:end],\n",
    "                    save_backend=save_backend,\n",
    "                    batch_size=batch_size,\n",
    "                    num_threads=num_threads,\n",
    "                    py_num_workers=py_num_workers,  # ram_v3\n",
    "                    py_start_method=py_start_method,\n",
    "                    device_id=worker_device_id,\n",
    "                ) for start, end, worker_device_id in zip(\n",
    "                    starts, ends, device_id))\n",
    "        else:  # manually start multiprocessing's processes\n",
    "            workers = []\n",
    "            daemon = False if py_num_workers > 0 else True\n",
    "            for i in range(parallel_n_jobs):\n",
    "                start = starts[i]\n",
    "                end = ends[i]\n",
    "                worker_device_id = device_id[i]\n",
    "                worker = mp.Process(group=None,\n",
    "                                    target=decode_crop_save_dali,\n",
    "                                    args=(\n",
    "                                        dcm_paths[start:end],\n",
    "                                        save_paths[start:end],\n",
    "                                    ),\n",
    "                                    kwargs={\n",
    "                                        'save_backend': save_backend,\n",
    "                                        'batch_size': batch_size,\n",
    "                                        'num_threads': num_threads,\n",
    "                                        'py_num_workers': py_num_workers,\n",
    "                                        'py_start_method': py_start_method,\n",
    "                                        'device_id': worker_device_id,\n",
    "                                    },\n",
    "                                    daemon=daemon)\n",
    "                workers.append(worker)\n",
    "            for worker in workers:\n",
    "                worker.start()\n",
    "            for worker in workers:\n",
    "                worker.join()\n",
    "    return\n",
    "\n",
    "\n",
    "def _single_decode_crop_save_sdl(roi_extractor,\n",
    "                                 dcm_path,\n",
    "                                 save_path,\n",
    "                                 save_backend='cv2',\n",
    "                                 index=0):\n",
    "    dcm = dicomsdl.open(dcm_path)\n",
    "    meta = DicomsdlMetadata(dcm)\n",
    "    info = dcm.getPixelDataInfo()\n",
    "    if info['SamplesPerPixel'] != 1:\n",
    "        raise RuntimeError('SamplesPerPixel != 1')\n",
    "    else:\n",
    "        shape = [info['Rows'], info['Cols']]\n",
    "\n",
    "    ori_dtype = info['dtype']\n",
    "    img = np.empty(shape, dtype=ori_dtype)\n",
    "    dcm.copyFrameData(index, img)\n",
    "    img_torch = torch.from_numpy(img.astype(np.int16)).cuda()\n",
    "\n",
    "#     img = np.empty(shape, dtype=np.int16)\n",
    "#     dcm.copyFrameData(index, img)\n",
    "#     img_torch = torch.from_numpy(img).cuda()  # int16\n",
    "\n",
    "    # YOLOX for ROI extraction\n",
    "    # @TODO: optimize and sync training vs inference pipeline:\n",
    "    # @current: uint16 --> min_max_norm --> uint8 --> resize (cv2) --> padding\n",
    "    # @faster?: uint16 --> resize (torch interpolate) --> min_max_norm --> uint8 --> padding\n",
    "    # @fastest?: uint16 --> VOILUT --> resize (torch interpolate) --> uint8 --> padding\n",
    "    img_yolox = min_max_scale(img_torch)\n",
    "    img_yolox = (img_yolox * 255)  # float32\n",
    "    # @TODO: subtract on large array --> should move after F.interpolate()\n",
    "    if meta.invert:\n",
    "        img_yolox = 255 - img_yolox\n",
    "    # YOLOX infer\n",
    "    try:\n",
    "        xyxy, _area_pct, _conf = roi_extractor.detect_single(img_yolox)\n",
    "        if xyxy is not None:\n",
    "            x0, y0, x1, y1 = xyxy\n",
    "            crop = img_torch[y0:y1, x0:x1]\n",
    "        else:\n",
    "            crop = img_torch\n",
    "    except:\n",
    "        print('ROI extract exception!')\n",
    "        crop = img_torch\n",
    "\n",
    "    # apply voi lut\n",
    "    if meta.window_widths:\n",
    "        crop = apply_windowing_torch(crop,\n",
    "                                     window_width=meta.window_widths[0],\n",
    "                                     window_center=meta.window_centers[0],\n",
    "                                     voi_func=meta.voilut_func,\n",
    "                                     y_min=0,\n",
    "                                     y_max=255)\n",
    "    else:\n",
    "        print('No windowing param!')\n",
    "        crop = min_max_scale(crop)\n",
    "        crop = crop * 255\n",
    "        \n",
    "    if meta.invert:\n",
    "        crop = 255 - crop\n",
    "    crop = resize_and_pad(crop, MODEL_INPUT_SIZE)\n",
    "    crop = crop.to(torch.uint8)\n",
    "    crop = crop.cpu().numpy()\n",
    "    save_img_to_file(save_path, crop, backend=save_backend)\n",
    "\n",
    "\n",
    "def decode_crop_save_sdl(dcm_paths, save_paths, save_backend='cv2'):\n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "    roi_detector = roi_extract.RoiExtractor(engine_path=ROI_YOLOX_ENGINE_PATH,\n",
    "                                            input_size=ROI_YOLOX_INPUT_SIZE,\n",
    "                                            num_classes=1,\n",
    "                                            conf_thres=ROI_YOLOX_CONF_THRES,\n",
    "                                            nms_thres=ROI_YOLOX_NMS_THRES,\n",
    "                                            class_agnostic=False,\n",
    "                                            area_pct_thres=ROI_AREA_PCT_THRES,\n",
    "                                            hw=ROI_YOLOX_HW,\n",
    "                                            strides=ROI_YOLOX_STRIDES,\n",
    "                                            exp=None)\n",
    "    print('ROI extractor (YOLOX) loaded!')\n",
    "    for i in tqdm(range(len(dcm_paths))):\n",
    "        _single_decode_crop_save_sdl(roi_detector, dcm_paths[i], save_paths[i],\n",
    "                                     save_backend)\n",
    "\n",
    "    del roi_detector\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return\n",
    "\n",
    "\n",
    "def decode_crop_save_sdl_parallel(dcm_paths,\n",
    "                                  save_paths,\n",
    "                                  save_backend='cv2',\n",
    "                                  parallel_n_jobs=2,\n",
    "                                  parallel_n_chunks = 4,\n",
    "                                  joblib_backend='loky'):\n",
    "    assert len(dcm_paths) == len(save_paths)\n",
    "    if parallel_n_jobs == 1:\n",
    "        print('No parralel. Starting the tasks within current process.')\n",
    "        return decode_crop_save_sdl(dcm_paths, save_paths, save_backend)\n",
    "    else:\n",
    "        num_samples = len(dcm_paths)\n",
    "        num_samples_per_chunk = num_samples // parallel_n_chunks\n",
    "        if num_samples % parallel_n_chunks > 0:\n",
    "            num_samples_per_chunk += 1\n",
    "        starts = [num_samples_per_chunk * i for i in range(parallel_n_chunks)]\n",
    "        ends = [\n",
    "            min(start + num_samples_per_chunk, num_samples)\n",
    "            for start in starts\n",
    "        ]\n",
    "\n",
    "        print(\n",
    "            f'Starting {parallel_n_jobs} jobs with backend `{joblib_backend}`, {parallel_n_chunks} chunks...'\n",
    "        )\n",
    "        _ = Parallel(n_jobs=parallel_n_jobs, backend=joblib_backend)(\n",
    "            delayed(decode_crop_save_sdl)(dcm_paths[start:end],\n",
    "                                          save_paths[start:end], save_backend)\n",
    "            for start, end in zip(starts, ends))\n",
    "        \n",
    "        \n",
    "def make_uid_transfer_dict(df, dcm_root_dir):\n",
    "    machine_id_to_transfer = {}\n",
    "    machine_id = df.machine_id.unique()\n",
    "    for i in machine_id:\n",
    "        row = df[df.machine_id == i].iloc[0]\n",
    "        sample_dcm_path = os.path.join(dcm_root_dir, str(row.patient_id),\n",
    "                                       f'{row.image_id}.dcm')\n",
    "        dicom = pydicom.dcmread(sample_dcm_path)\n",
    "        machine_id_to_transfer[i] = dicom.file_meta.TransferSyntaxUID\n",
    "    return machine_id_to_transfer\n",
    "\n",
    "\n",
    "class ValTransform:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.transform_fn = A.Compose([ToTensorV2(transpose_mask=True)])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.transform_fn(image=img)['image']\n",
    "\n",
    "\n",
    "class RSNADataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, img_root_dir, transform_fn=None):\n",
    "        self.img_paths = []\n",
    "        self.transform_fn = transform_fn\n",
    "        self.df = df\n",
    "        for i in tqdm(range(len(df))):\n",
    "            patient_id = df.at[i, 'patient_id']\n",
    "            image_id = df.at[i, 'image_id']\n",
    "            img_name = f'{patient_id}@{image_id}.png'\n",
    "            img_path = os.path.join(img_root_dir, img_name)\n",
    "            self.img_paths.append(img_path)\n",
    "        print(f'Done loading dataset with {len(self.img_paths)} samples.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print('ERROR:', img_path)\n",
    "        if self.transform_fn:\n",
    "            img = self.transform_fn(img)\n",
    "        return img\n",
    "\n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "    \n",
    "\n",
    "def rescale_probs(probs, thres):\n",
    "    # [0, thres] --> [0, 0.5]\n",
    "    # [thres, 1] --> [0.5, 1]\n",
    "    probs = probs.copy()\n",
    "    probs[probs <= thres] = 0.5 + (probs - thres)/thres * 0.5\n",
    "    probs[probs > thres] = 0.5 + (probs - thres) / (1 - thres) * 0.5\n",
    "    return probs\n",
    "\n",
    "\n",
    "######################################################\n",
    "# MAIN CODE\n",
    "\n",
    "if MODE == 'KAGGLE-TEST':\n",
    "    global_df = pd.read_csv(CSV_PATH)\n",
    "else:\n",
    "    global_df = pd.read_csv(CSV_PATH)[:500]\n",
    "MACHINE_TO_SUID = make_uid_transfer_dict(global_df, DCM_ROOT_DIR)\n",
    "all_patients = list(global_df.patient_id.unique())\n",
    "num_patients = len(all_patients)\n",
    "\n",
    "num_patients_per_chunk = num_patients // N_CHUNKS + 1\n",
    "chunk_patients = [\n",
    "    all_patients[num_patients_per_chunk * i:num_patients_per_chunk * (i + 1)]\n",
    "    for i in range(N_CHUNKS)\n",
    "]\n",
    "print(f'PATIENT CHUNKS: {[len(c) for c in chunk_patients]}')\n",
    "\n",
    "pred_dfs = []\n",
    "for chunk_idx, chunk_patients in enumerate(chunk_patients):\n",
    "    os.makedirs(SAVE_IMG_ROOT_DIR, exist_ok=True)\n",
    "    df = global_df[global_df.patient_id.isin(chunk_patients)].reset_index(\n",
    "        drop=True)\n",
    "    print(\n",
    "        f'Processing chunk {chunk_idx} with {len(chunk_patients)} patients, {len(df)} images'\n",
    "    )\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "    dcm_paths = []\n",
    "    save_paths = []\n",
    "    dali_dcm_paths = []\n",
    "    dali_save_paths = []\n",
    "    for i in range(len(df)):\n",
    "        patient_id = df.at[i, 'patient_id']\n",
    "        image_id = df.at[i, 'image_id']\n",
    "        suid = MACHINE_TO_SUID[df.at[i, 'machine_id']]\n",
    "        dcm_path = os.path.join(DCM_ROOT_DIR, str(patient_id),\n",
    "                                f'{image_id}.dcm')\n",
    "        save_path = os.path.join(SAVE_IMG_ROOT_DIR,\n",
    "                                 f'{patient_id}@{image_id}.png')\n",
    "        # if os.path.isfile(save_path):\n",
    "        #     continue\n",
    "        dcm_paths.append(dcm_path)\n",
    "        save_paths.append(save_path)\n",
    "        if suid == J2K_SUID or suid == JLL_SUID:\n",
    "            dali_dcm_paths.append(dcm_path)\n",
    "            dali_save_paths.append(save_path)\n",
    "    if 1:\n",
    "        t0 = time.time()\n",
    "        # try to decode all .90 and .70 with DALI\n",
    "        decode_and_save_dali_parallel(\n",
    "            dali_dcm_paths,\n",
    "            dali_save_paths,\n",
    "            save_backend='cv2',\n",
    "            batch_size=1,\n",
    "            num_threads=1,\n",
    "            py_num_workers=0,\n",
    "            py_start_method='fork',\n",
    "            device_id=0,\n",
    "            parallel_n_jobs=N_CPUS + 1,\n",
    "            parallel_n_chunks = N_CPUS + 1,\n",
    "            parallel_backend='joblib',  # joblib or multiprocessing\n",
    "            joblib_backend='loky')\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        t1 = time.time()\n",
    "        print(f'DALI done in {t1 - t0} sec')\n",
    "\n",
    "\n",
    "        # CPU decode all others (exceptions) with dicomsdl\n",
    "        done_img_names = os.listdir(SAVE_IMG_ROOT_DIR)\n",
    "        save_img_names = [os.path.basename(p) for p in save_paths]\n",
    "        remain_img_names = list(set(save_img_names) - set(done_img_names))\n",
    "        remain_img_paths = [\n",
    "            os.path.join(SAVE_IMG_ROOT_DIR, name) for name in remain_img_names\n",
    "        ]\n",
    "        remain_dcm_paths = []\n",
    "        for name in remain_img_names:\n",
    "            patient_id, image_id = os.path.basename(name).split('.')[0].split(\n",
    "                '@')\n",
    "            remain_dcm_paths.append(\n",
    "                os.path.join(DCM_ROOT_DIR, patient_id, f'{image_id}.dcm'))\n",
    "        num_remain = len(remain_dcm_paths)\n",
    "        print(f'Number of undecoded files: {num_remain}')\n",
    "#         print(f'Remains: {remain_img_names}')\n",
    "        if num_remain > 0:\n",
    "            # 16 or just any > 0 number\n",
    "            if num_remain > 32 * N_CPUS:\n",
    "                sdl_n_jobs = N_CPUS\n",
    "                sdl_n_chunks = N_CPUS\n",
    "            else:\n",
    "                sdl_n_jobs = 1\n",
    "                sdl_n_chunks = 1\n",
    "            decode_crop_save_sdl_parallel(remain_dcm_paths,\n",
    "                                          remain_img_paths,\n",
    "                                          save_backend='cv2',\n",
    "                                          parallel_n_jobs=sdl_n_jobs,\n",
    "                                          parallel_n_chunks = sdl_n_chunks,\n",
    "                                          joblib_backend='loky')\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            print('No remain files to decode.')\n",
    "        t2 = time.time()\n",
    "        print(f'SDL done in { t2 - t1} sec')\n",
    "        print(f'TOTAL DECODING TIME: {t2 - t0} sec')\n",
    "\n",
    "    dataset = RSNADataset(df, SAVE_IMG_ROOT_DIR, transform_fn=ValTransform())\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    do_rescale = False\n",
    "    thresholds = [0.25, 0.34, 0.35, 0.31]\n",
    "    \n",
    "    wrap_autocast = False\n",
    "    model = TRTModule()\n",
    "    model.load_state_dict(torch.load('/kaggle/input/rsna-final-convnext-small-fp32/torch2trt_convnext_small_b2_fp32.engine'))\n",
    "\n",
    "#     ############# TORCH MODEL ############\n",
    "#     wrap_autocast = True\n",
    "# #     MODEL_NAME = 'tf_efficientnet_b4.ns_jft_in1k'\n",
    "#     MODEL_NAME = 'convnext_small.fb_in22k_ft_in1k_384'\n",
    "#     MODEL_CKPTS = [\n",
    "#         '/kaggle/input/kaggle-rsna-ckpts/fold0_convnext-small_exp4_ratio8_ep11.pth.tar',\n",
    "#     ]\n",
    "#     NUM_CLASSES = 2\n",
    "#     IN_CHANS = 3\n",
    "#     MEAN = np.array([0.485, 0.456, 0.406]) * 255\n",
    "#     STD = np.array([0.229, 0.224, 0.225]) * 255\n",
    "#     GLOBAL_POOL = 'avg'\n",
    "#     from convert_model import KFoldEnsembleModel\n",
    "#     from functools import partial\n",
    "#     AMP_DTYPE = torch.float16\n",
    "#     # AMP_DTYPE = torch.bfloat16\n",
    "#     amp_autocast = partial(torch.autocast, device_type='cuda', dtype=AMP_DTYPE)\n",
    "#     model = KFoldEnsembleModel(MODEL_NAME, NUM_CLASSES, IN_CHANS, MODEL_CKPTS)\n",
    "#     model.eval()\n",
    "#     model.cuda()\n",
    "#     ############ TORCH MODEL ################\n",
    "\n",
    "    all_probs = []\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = batch.cuda().float()\n",
    "            if wrap_autocast:\n",
    "                with amp_autocast():\n",
    "                    probs = model(batch)\n",
    "            else:\n",
    "                probs = model(batch)\n",
    "            probs = probs.cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "    \n",
    "    # N * num_models\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_probs = np.nan_to_num(all_probs, nan=0.0, posinf=None, neginf=None)\n",
    "    if do_rescale:\n",
    "        for j in range(all_probs.shape[1]):\n",
    "            all_probs[:, j] = rescale_probs(all_probs[:, j], thresholds[j])\n",
    "    all_probs = all_probs.mean(axis=-1)\n",
    "    assert all_probs.shape[0] == len(df)\n",
    "\n",
    "    df['preds'] = all_probs\n",
    "    pred_dfs.append(df)\n",
    "\n",
    "    print(f'DONE CHUNK {chunk_idx} with {len(df)} samples')\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if RM_DONE_CHUNK:\n",
    "        shutil.rmtree(SAVE_IMG_ROOT_DIR)\n",
    "        print(f'Removed save image directory {SAVE_IMG_ROOT_DIR}')\n",
    "    print('-----------------------------\\n\\n')\n",
    "\n",
    "    \n",
    "pred_df = pd.concat(pred_dfs).reset_index(drop=True)\n",
    "if 'prediction_id' not in pred_df.columns:\n",
    "    pred_df['prediction_id'] = pred_df.apply(lambda row: str(row.patient_id) + '_' + row.laterality, axis = 1)\n",
    "submit_df = pred_df[['prediction_id', 'preds']]\n",
    "submit_df = pred_df.groupby('prediction_id').mean()\n",
    "if AUTO_THRES:\n",
    "    thres = np.quantile(submit_df['preds'].values, AUTO_THRES_PERCENTILE)\n",
    "else:\n",
    "    thres = THRES\n",
    "submit_df['cancer'] = (submit_df['preds'].values > thres).astype(int)\n",
    "submit_df = submit_df['cancer']\n",
    "submit_df.to_csv('submission.csv')\n",
    "    \n",
    "if MODE == 'KAGGLE-TEST':\n",
    "    pass\n",
    "else:\n",
    "    pred_df['targets'] = pred_df['cancer']\n",
    "    pred_df.to_csv('prediction.csv')\n",
    "    metrics = compute_metrics(pred_df,\n",
    "                              plot_save_path='metric_plot.png',\n",
    "                              thres_range=(0, 1, 0.01),\n",
    "                              sort_by='pfbeta',\n",
    "                              additional_info=True)\n",
    "    print('METRICS:', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263f589",
   "metadata": {
    "papermill": {
     "duration": 0.012439,
     "end_time": "2023-02-27T15:15:00.314879",
     "exception": false,
     "start_time": "2023-02-27T15:15:00.302440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92d76d5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:15:00.341724Z",
     "iopub.status.busy": "2023-02-27T15:15:00.340818Z",
     "iopub.status.idle": "2023-02-27T15:15:00.345815Z",
     "shell.execute_reply": "2023-02-27T15:15:00.344938Z"
    },
    "papermill": {
     "duration": 0.020548,
     "end_time": "2023-02-27T15:15:00.347840",
     "exception": false,
     "start_time": "2023-02-27T15:15:00.327292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !ls /kaggle/tmp/pngs  | wc -l\n",
    "# !du -sh /kaggle/tmp/pngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c143a637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:15:00.374382Z",
     "iopub.status.busy": "2023-02-27T15:15:00.374081Z",
     "iopub.status.idle": "2023-02-27T15:15:00.379069Z",
     "shell.execute_reply": "2023-02-27T15:15:00.377877Z"
    },
    "papermill": {
     "duration": 0.020971,
     "end_time": "2023-02-27T15:15:00.381279",
     "exception": false,
     "start_time": "2023-02-27T15:15:00.360308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if MODE != 'KAGGLE-TEST':    \n",
    "#     from IPython.display import Image, display as idisplay\n",
    "#     import PIL\n",
    "#     from PIL import Image\n",
    "\n",
    "\n",
    "#     def longest_resize(img, resize = 416):\n",
    "#         h, w = img.shape[:2]\n",
    "#         r = resize / max(h, w)\n",
    "#         new_h, new_w = int(r * h), int(r * w)\n",
    "#         img = cv2.resize(img, (new_w, new_h))\n",
    "#         return img\n",
    "\n",
    "#     def display(img, resize = None):\n",
    "#     #     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         if resize:\n",
    "#             img = longest_resize(img, resize)\n",
    "#         idisplay(Image.fromarray(img))\n",
    "#         print('\\n--------------------------\\n')\n",
    "\n",
    "#     names = os.listdir(SAVE_IMG_ROOT_DIR)\n",
    "#     img_paths = [os.path.join(SAVE_IMG_ROOT_DIR, name) for name in names]\n",
    "\n",
    "#     for img_path in img_paths[:2]:\n",
    "#         print(img_path)\n",
    "#         img = cv2.imread(img_path)\n",
    "#         display(img, resize = 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9f1d17a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:15:00.407604Z",
     "iopub.status.busy": "2023-02-27T15:15:00.407331Z",
     "iopub.status.idle": "2023-02-27T15:15:00.412375Z",
     "shell.execute_reply": "2023-02-27T15:15:00.411535Z"
    },
    "papermill": {
     "duration": 0.020796,
     "end_time": "2023-02-27T15:15:00.414514",
     "exception": false,
     "start_time": "2023-02-27T15:15:00.393718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# img = cv2.imread(img_path)\n",
    "# display(img, resize = 640)\n",
    "# img = torch.from_numpy(img).cuda()\n",
    "# img = img.unsqueeze(0).permute(0, 3, 1, 2)\n",
    "# print(img.shape)\n",
    "# img_flip = torch.flip(img, [-1])\n",
    "# print(img_flip.shape)\n",
    "# img_flip = img_flip.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "# display(img_flip, resize = 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b63c8",
   "metadata": {
    "papermill": {
     "duration": 0.012392,
     "end_time": "2023-02-27T15:15:00.439947",
     "exception": false,
     "start_time": "2023-02-27T15:15:00.427555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f8957f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:15:00.467608Z",
     "iopub.status.busy": "2023-02-27T15:15:00.467346Z",
     "iopub.status.idle": "2023-02-27T15:15:00.471383Z",
     "shell.execute_reply": "2023-02-27T15:15:00.470303Z"
    },
    "papermill": {
     "duration": 0.020669,
     "end_time": "2023-02-27T15:15:00.473674",
     "exception": false,
     "start_time": "2023-02-27T15:15:00.453005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     import onnx_graphsurgeon\n",
    "# except:\n",
    "#     !pip install onnxruntime-gpu\n",
    "#     !pip install onnx_graphsurgeon --index-url https://pypi.ngc.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94c7a3",
   "metadata": {
    "papermill": {
     "duration": 0.012292,
     "end_time": "2023-02-27T15:15:00.498459",
     "exception": false,
     "start_time": "2023-02-27T15:15:00.486167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "719622b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:15:00.535224Z",
     "iopub.status.busy": "2023-02-27T15:15:00.534842Z",
     "iopub.status.idle": "2023-02-27T15:15:00.539388Z",
     "shell.execute_reply": "2023-02-27T15:15:00.538389Z"
    },
    "papermill": {
     "duration": 0.030265,
     "end_time": "2023-02-27T15:15:00.541166",
     "exception": false,
     "start_time": "2023-02-27T15:15:00.510901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fda8c87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T15:15:00.567797Z",
     "iopub.status.busy": "2023-02-27T15:15:00.567531Z",
     "iopub.status.idle": "2023-02-27T15:15:00.571588Z",
     "shell.execute_reply": "2023-02-27T15:15:00.570537Z"
    },
    "papermill": {
     "duration": 0.019898,
     "end_time": "2023-02-27T15:15:00.573816",
     "exception": false,
     "start_time": "2023-02-27T15:15:00.553918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     del model, sample_inp\n",
    "# except:\n",
    "#     pass\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615ce73",
   "metadata": {
    "papermill": {
     "duration": 0.012401,
     "end_time": "2023-02-27T15:15:00.598844",
     "exception": false,
     "start_time": "2023-02-27T15:15:00.586443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39926f59",
   "metadata": {
    "papermill": {
     "duration": 0.012561,
     "end_time": "2023-02-27T15:15:00.624108",
     "exception": false,
     "start_time": "2023-02-27T15:15:00.611547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684dd6c2",
   "metadata": {
    "papermill": {
     "duration": 0.012739,
     "end_time": "2023-02-27T15:15:00.649459",
     "exception": false,
     "start_time": "2023-02-27T15:15:00.636720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 331.900492,
   "end_time": "2023-02-27T15:15:03.382656",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-27T15:09:31.482164",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
