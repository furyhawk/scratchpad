{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/train/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import tempfile\n",
    "import logging\n",
    "import logging.config\n",
    "\n",
    "import contextlib\n",
    "import hashlib\n",
    "from itertools import repeat\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import ExifTags, Image, ImageOps\n",
    "import torch\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGING_NAME = \"ultralytics\"\n",
    "VERBOSE = str(os.getenv(\"YOLO_VERBOSE\", True)).lower() == \"true\"  # global verbose mode\n",
    "\n",
    "\n",
    "def set_logging(name=LOGGING_NAME, verbose=True):\n",
    "    # sets up logging for the given name\n",
    "    rank = int(os.getenv(\"RANK\", -1))  # rank in world for Multi-GPU trainings\n",
    "    level = logging.INFO if verbose and rank in {-1, 0} else logging.ERROR\n",
    "    logging.config.dictConfig(\n",
    "        {\n",
    "            \"version\": 1,\n",
    "            \"disable_existing_loggers\": False,\n",
    "            \"formatters\": {name: {\"format\": \"%(message)s\"}},\n",
    "            \"handlers\": {\n",
    "                name: {\n",
    "                    \"class\": \"logging.StreamHandler\",\n",
    "                    \"formatter\": name,\n",
    "                    \"level\": level,\n",
    "                }\n",
    "            },\n",
    "            \"loggers\": {name: {\"level\": level, \"handlers\": [name], \"propagate\": False}},\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Set logger\n",
    "set_logging(LOGGING_NAME, verbose=VERBOSE)  # run before defining LOGGER\n",
    "LOGGER = logging.getLogger(\n",
    "    LOGGING_NAME\n",
    ")  # define globally (used in train.py, val.py, detect.py, etc.)\n",
    "\n",
    "NUM_THREADS = min(\n",
    "    8, max(1, os.cpu_count() - 1)\n",
    ")  # number of YOLOv5 multiprocessing threads\n",
    "TQDM_BAR_FORMAT = \"{l_bar}{bar:10}{r_bar}\"  # tqdm bar format\n",
    "IMG_FORMATS = (\n",
    "    \"bmp\",\n",
    "    \"dng\",\n",
    "    \"jpeg\",\n",
    "    \"jpg\",\n",
    "    \"mpo\",\n",
    "    \"png\",\n",
    "    \"tif\",\n",
    "    \"tiff\",\n",
    "    \"webp\",\n",
    "    \"pfm\",\n",
    ")  # include image suffixes\n",
    "LOCAL_RANK = int(\n",
    "    os.getenv(\"LOCAL_RANK\", -1)\n",
    ")  # https://pytorch.org/docs/stable/elastic/run.html\n",
    "RANK = int(os.getenv(\"RANK\", -1))\n",
    "PIN_MEMORY = str(os.getenv('PIN_MEMORY', True)).lower() == 'true'  # global pin_memory for dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    \"\"\"Base Dataset.\n",
    "    Args:\n",
    "        img_path (str): image path.\n",
    "        pipeline (dict): a dict of image transforms.\n",
    "        label_path (str): label path, this can also be an ann_file or other custom label path.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_path,\n",
    "        imgsz=640,\n",
    "        cache=False,\n",
    "        augment=True,\n",
    "        hyp=None,\n",
    "        prefix=\"\",\n",
    "        rect=False,\n",
    "        batch_size=None,\n",
    "        stride=32,\n",
    "        pad=0.5,\n",
    "        single_cls=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_path = img_path\n",
    "        self.imgsz = imgsz\n",
    "        self.augment = augment\n",
    "        self.single_cls = single_cls\n",
    "        self.prefix = prefix\n",
    "\n",
    "        self.im_files = self.get_img_files(self.img_path)\n",
    "        self.labels = self.get_labels()\n",
    "        if self.single_cls:\n",
    "            self.update_labels(include_class=[])\n",
    "\n",
    "        self.ni = len(self.labels)\n",
    "\n",
    "        # rect stuff\n",
    "        self.rect = rect\n",
    "        self.batch_size = batch_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        if self.rect:\n",
    "            assert self.batch_size is not None\n",
    "            self.set_rectangle()\n",
    "\n",
    "        # cache stuff\n",
    "        self.ims = [None] * self.ni\n",
    "        self.npy_files = [Path(f).with_suffix(\".npy\") for f in self.im_files]\n",
    "        if cache:\n",
    "            self.cache_images(cache)\n",
    "\n",
    "        # transforms\n",
    "        # self.transforms = self.build_transforms(hyp=hyp)\n",
    "\n",
    "    def get_img_files(self, img_path):\n",
    "        \"\"\"Read image files.\"\"\"\n",
    "        try:\n",
    "            f = []  # image files\n",
    "            for p in img_path if isinstance(img_path, list) else [img_path]:\n",
    "                p = Path(p)  # os-agnostic\n",
    "                if p.is_dir():  # dir\n",
    "                    f += glob.glob(str(p / \"**\" / \"*.*\"), recursive=True)\n",
    "                    # f = list(p.rglob('*.*'))  # pathlib\n",
    "                elif p.is_file():  # file\n",
    "                    with open(p) as t:\n",
    "                        t = t.read().strip().splitlines()\n",
    "                        parent = str(p.parent) + os.sep\n",
    "                        f += [\n",
    "                            x.replace(\"./\", parent) if x.startswith(\"./\") else x\n",
    "                            for x in t\n",
    "                        ]  # local to global path\n",
    "                        # f += [p.parent / x.lstrip(os.sep) for x in t]  # local to global path (pathlib)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"{self.prefix}{p} does not exist\")\n",
    "            im_files = sorted(\n",
    "                x.replace(\"/\", os.sep)\n",
    "                for x in f\n",
    "                if x.split(\".\")[-1].lower() in IMG_FORMATS\n",
    "            )\n",
    "            # self.img_files = sorted([x for x in f if x.suffix[1:].lower() in IMG_FORMATS])  # pathlib\n",
    "            assert im_files, f\"{self.prefix}No images found\"\n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError(\n",
    "                f\"{self.prefix}Error loading data from {img_path}\\n\"\n",
    "            ) from e\n",
    "        return im_files\n",
    "\n",
    "    def update_labels(self, include_class: Optional[list]):\n",
    "        \"\"\"include_class, filter labels to include only these classes (optional)\"\"\"\n",
    "        include_class_array = np.array(include_class).reshape(1, -1)\n",
    "        for i in range(len(self.labels)):\n",
    "            if include_class:\n",
    "                cls = self.labels[i][\"cls\"]\n",
    "                bboxes = self.labels[i][\"bboxes\"]\n",
    "                segments = self.labels[i][\"segments\"]\n",
    "                j = (cls == include_class_array).any(1)\n",
    "                self.labels[i][\"cls\"] = cls[j]\n",
    "                self.labels[i][\"bboxes\"] = bboxes[j]\n",
    "                if segments:\n",
    "                    self.labels[i][\"segments\"] = segments[j]\n",
    "            if self.single_cls:\n",
    "                self.labels[i][\"cls\"][:, 0] = 0\n",
    "\n",
    "    def load_image(self, i):\n",
    "        # Loads 1 image from dataset index 'i', returns (im, resized hw)\n",
    "        im, f, fn = self.ims[i], self.im_files[i], self.npy_files[i]\n",
    "        if im is None:  # not cached in RAM\n",
    "            if fn.exists():  # load npy\n",
    "                im = np.load(fn)\n",
    "            else:  # read image\n",
    "                im = cv2.imread(f)  # BGR\n",
    "                if im is None:\n",
    "                    raise FileNotFoundError(f\"Image Not Found {f}\")\n",
    "            h0, w0 = im.shape[:2]  # orig hw\n",
    "            r = self.imgsz / max(h0, w0)  # ratio\n",
    "            if r != 1:  # if sizes are not equal\n",
    "                interp = cv2.INTER_LINEAR if (self.augment or r > 1) else cv2.INTER_AREA\n",
    "                im = cv2.resize(\n",
    "                    im, (math.ceil(w0 * r), math.ceil(h0 * r)), interpolation=interp\n",
    "                )\n",
    "            return im, (h0, w0), im.shape[:2]  # im, hw_original, hw_resized\n",
    "        return self.ims[i], self.im_hw0[i], self.im_hw[i]  # im, hw_original, hw_resized\n",
    "\n",
    "    def cache_images(self, cache):\n",
    "        # cache images to memory or disk\n",
    "        gb = 0  # Gigabytes of cached images\n",
    "        self.im_hw0, self.im_hw = [None] * self.ni, [None] * self.ni\n",
    "        fcn = self.cache_images_to_disk if cache == \"disk\" else self.load_image\n",
    "        with ThreadPool(NUM_THREADS) as pool:\n",
    "            results = pool.imap(fcn, range(self.ni))\n",
    "            pbar = tqdm(\n",
    "                enumerate(results),\n",
    "                total=self.ni,\n",
    "                bar_format=TQDM_BAR_FORMAT,\n",
    "                disable=LOCAL_RANK > 0,\n",
    "            )\n",
    "            for i, x in pbar:\n",
    "                if cache == \"disk\":\n",
    "                    gb += self.npy_files[i].stat().st_size\n",
    "                else:  # 'ram'\n",
    "                    (\n",
    "                        self.ims[i],\n",
    "                        self.im_hw0[i],\n",
    "                        self.im_hw[i],\n",
    "                    ) = x  # im, hw_orig, hw_resized = load_image(self, i)\n",
    "                    gb += self.ims[i].nbytes\n",
    "                pbar.desc = f\"{self.prefix}Caching images ({gb / 1E9:.1f}GB {cache})\"\n",
    "            pbar.close()\n",
    "\n",
    "    def cache_images_to_disk(self, i):\n",
    "        # Saves an image as an *.npy file for faster loading\n",
    "        f = self.npy_files[i]\n",
    "        if not f.exists():\n",
    "            np.save(f.as_posix(), cv2.imread(self.im_files[i]))\n",
    "\n",
    "    def set_rectangle(self):\n",
    "        bi = np.floor(np.arange(self.ni) / self.batch_size).astype(int)  # batch index\n",
    "        nb = bi[-1] + 1  # number of batches\n",
    "\n",
    "        s = np.array([x.pop(\"shape\") for x in self.labels])  # hw\n",
    "        ar = s[:, 0] / s[:, 1]  # aspect ratio\n",
    "        irect = ar.argsort()\n",
    "        self.im_files = [self.im_files[i] for i in irect]\n",
    "        self.labels = [self.labels[i] for i in irect]\n",
    "        ar = ar[irect]\n",
    "\n",
    "        # Set training image shapes\n",
    "        shapes = [[1, 1]] * nb\n",
    "        for i in range(nb):\n",
    "            ari = ar[bi == i]\n",
    "            mini, maxi = ari.min(), ari.max()\n",
    "            if maxi < 1:\n",
    "                shapes[i] = [maxi, 1]\n",
    "            elif mini > 1:\n",
    "                shapes[i] = [1, 1 / mini]\n",
    "\n",
    "        self.batch_shapes = (\n",
    "            np.ceil(np.array(shapes) * self.imgsz / self.stride + self.pad).astype(int)\n",
    "            * self.stride\n",
    "        )\n",
    "        self.batch = bi  # batch index of image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_label_info(index)  # self.transforms(self.get_label_info(index))\n",
    "\n",
    "    def get_label_info(self, index):\n",
    "        label = self.labels[index].copy()\n",
    "        label.pop(\"shape\", None)  # shape is for rect, remove it\n",
    "        label[\"img\"], label[\"ori_shape\"], label[\"resized_shape\"] = self.load_image(\n",
    "            index\n",
    "        )\n",
    "        label[\"ratio_pad\"] = (\n",
    "            label[\"resized_shape\"][0] / label[\"ori_shape\"][0],\n",
    "            label[\"resized_shape\"][1] / label[\"ori_shape\"][1],\n",
    "        )  # for evaluation\n",
    "        if self.rect:\n",
    "            label[\"rect_shape\"] = self.batch_shapes[self.batch[index]]\n",
    "        label = self.update_labels_info(label)\n",
    "        return label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def update_labels_info(self, label):\n",
    "        \"\"\"custom your label format here\"\"\"\n",
    "        return label\n",
    "\n",
    "    # def build_transforms(self, hyp=None):\n",
    "    #     \"\"\"Users can custom augmentations here\n",
    "    #     like:\n",
    "    #         if self.augment:\n",
    "    #             # training transforms\n",
    "    #             return Compose([])\n",
    "    #         else:\n",
    "    #             # val transforms\n",
    "    #             return Compose([])\n",
    "    #     \"\"\"\n",
    "    #     raise NotImplementedError\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Users can custom their own format here.\n",
    "        Make sure your output is a list with each element like below:\n",
    "            dict(\n",
    "                im_file=im_file,\n",
    "                shape=shape,  # format: (height, width)\n",
    "                cls=cls,\n",
    "                bboxes=bboxes, # xywh\n",
    "                segments=segments,  # xy\n",
    "                keypoints=keypoints, # xy\n",
    "                normalized=True, # or False\n",
    "                bbox_format=\"xyxy\",  # or xywh, ltwh\n",
    "            )\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2label_paths(img_paths):\n",
    "    # Define label paths as a function of image paths\n",
    "    sa, sb = (\n",
    "        f\"{os.sep}images{os.sep}\",\n",
    "        f\"{os.sep}labels{os.sep}\",\n",
    "    )  # /images/, /labels/ substrings\n",
    "    return [sb.join(x.rsplit(sa, 1)).rsplit(\".\", 1)[0] + \".txt\" for x in img_paths]\n",
    "\n",
    "\n",
    "def get_hash(paths):\n",
    "    # Returns a single hash value of a list of paths (files or dirs)\n",
    "    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes\n",
    "    h = hashlib.sha256(str(size).encode())  # hash sizes\n",
    "    h.update(\"\".join(paths).encode())  # hash paths\n",
    "    return h.hexdigest()  # return hash\n",
    "\n",
    "def seed_worker(worker_id):  # noqa\n",
    "    # Set dataloader worker seed https://pytorch.org/docs/stable/notes/randomness.html#dataloader\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get orientation exif tag\n",
    "\n",
    "\n",
    "for orientation in ExifTags.TAGS.keys():\n",
    "    if ExifTags.TAGS[orientation] == \"Orientation\":\n",
    "        break\n",
    "\n",
    "\n",
    "def exif_size(img):\n",
    "    # Returns exif-corrected PIL size\n",
    "    s = img.size  # (width, height)\n",
    "    with contextlib.suppress(Exception):\n",
    "        rotation = dict(img._getexif().items())[orientation]\n",
    "        if rotation in [6, 8]:  # rotation 270 or 90\n",
    "            s = (s[1], s[0])\n",
    "    return s\n",
    "\n",
    "\n",
    "def is_dir_writeable(dir_path: Union[str, Path]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a directory is writeable.\n",
    "\n",
    "    Args:\n",
    "        dir_path (str) or (Path): The path to the directory.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the directory is writeable, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with tempfile.TemporaryFile(dir=dir_path):\n",
    "            pass\n",
    "        return True\n",
    "    except OSError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def xyxy2xywh(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray) or (torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "    Returns:\n",
    "       y (np.ndarray) or (torch.Tensor): The bounding box coordinates in (x, y, width, height) format.\n",
    "    \"\"\"\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n",
    "    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n",
    "    y[..., 2] = x[..., 2] - x[..., 0]  # width\n",
    "    y[..., 3] = x[..., 3] - x[..., 1]  # height\n",
    "    return y\n",
    "\n",
    "\n",
    "def segments2boxes(segments):\n",
    "    \"\"\"\n",
    "    It converts segment labels to box labels, i.e. (cls, xy1, xy2, ...) to (cls, xywh)\n",
    "\n",
    "    Args:\n",
    "      segments (list): list of segments, each segment is a list of points, each point is a list of x, y coordinates\n",
    "\n",
    "    Returns:\n",
    "      (np.ndarray): the xywh coordinates of the bounding boxes.\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    for s in segments:\n",
    "        x, y = s.T  # segment xy\n",
    "        boxes.append([x.min(), y.min(), x.max(), y.max()])  # cls, xyxy\n",
    "    return xyxy2xywh(np.array(boxes))  # cls, xywh\n",
    "\n",
    "\n",
    "def verify_image_label(args):\n",
    "    # Verify one image-label pair\n",
    "    im_file, lb_file, prefix, keypoint, num_cls = args\n",
    "    keypoint = False\n",
    "    # number (missing, found, empty, corrupt), message, segments, keypoints\n",
    "    nm, nf, ne, nc, msg, segments, keypoints = 0, 0, 0, 0, \"\", [], None\n",
    "    try:\n",
    "        # verify images\n",
    "        im = Image.open(im_file)\n",
    "        im.verify()  # PIL verify\n",
    "        shape = exif_size(im)  # image size\n",
    "        shape = (shape[1], shape[0])  # hw\n",
    "        assert (shape[0] > 9) & (shape[1] > 9), f\"image size {shape} <10 pixels\"\n",
    "        assert im.format.lower() in IMG_FORMATS, f\"invalid image format {im.format}\"\n",
    "        if im.format.lower() in (\"jpg\", \"jpeg\"):\n",
    "            with open(im_file, \"rb\") as f:\n",
    "                f.seek(-2, 2)\n",
    "                if f.read() != b\"\\xff\\xd9\":  # corrupt JPEG\n",
    "                    ImageOps.exif_transpose(Image.open(im_file)).save(\n",
    "                        im_file, \"JPEG\", subsampling=0, quality=100\n",
    "                    )\n",
    "                    msg = (\n",
    "                        f\"{prefix}WARNING ⚠️ {im_file}: corrupt JPEG restored and saved\"\n",
    "                    )\n",
    "\n",
    "        # verify labels\n",
    "        if os.path.isfile(lb_file):\n",
    "            nf = 1  # label found\n",
    "            with open(lb_file) as f:\n",
    "                lb = [x.split() for x in f.read().strip().splitlines() if len(x)]\n",
    "                if any(len(x) > 6 for x in lb) and (not keypoint):  # is segment\n",
    "                    classes = np.array([x[0] for x in lb], dtype=np.float32)\n",
    "                    segments = [\n",
    "                        np.array(x[1:], dtype=np.float32).reshape(-1, 2) for x in lb\n",
    "                    ]  # (cls, xy1...)\n",
    "                    lb = np.concatenate(\n",
    "                        (classes.reshape(-1, 1), segments2boxes(segments)), 1\n",
    "                    )  # (cls, xywh)\n",
    "                lb = np.array(lb, dtype=np.float32)\n",
    "            nl = len(lb)\n",
    "            if nl:\n",
    "                if keypoint:\n",
    "                    assert lb.shape[1] == 56, \"labels require 56 columns each\"\n",
    "                    assert (\n",
    "                        lb[:, 5::3] <= 1\n",
    "                    ).all(), \"non-normalized or out of bounds coordinate labels\"\n",
    "                    assert (\n",
    "                        lb[:, 6::3] <= 1\n",
    "                    ).all(), \"non-normalized or out of bounds coordinate labels\"\n",
    "                    kpts = np.zeros((lb.shape[0], 39))\n",
    "                    for i in range(len(lb)):\n",
    "                        kpt = np.delete(\n",
    "                            lb[i, 5:], np.arange(2, lb.shape[1] - 5, 3)\n",
    "                        )  # remove occlusion param from GT\n",
    "                        kpts[i] = np.hstack((lb[i, :5], kpt))\n",
    "                    lb = kpts\n",
    "                    assert (\n",
    "                        lb.shape[1] == 39\n",
    "                    ), \"labels require 39 columns each after removing occlusion parameter\"\n",
    "                else:\n",
    "                    assert (\n",
    "                        lb.shape[1] == 5\n",
    "                    ), f\"labels require 5 columns, {lb.shape[1]} columns detected\"\n",
    "                    assert (\n",
    "                        lb[:, 1:] <= 1\n",
    "                    ).all(), f\"non-normalized or out of bounds coordinates {lb[:, 1:][lb[:, 1:] > 1]}\"\n",
    "                # All labels\n",
    "                max_cls = int(lb[:, 0].max())  # max label count\n",
    "                assert max_cls <= num_cls, (\n",
    "                    f\"Label class {max_cls} exceeds dataset class count {num_cls}. \"\n",
    "                    f\"Possible class labels are 0-{num_cls - 1}{lb}\"\n",
    "                )\n",
    "                assert (lb >= 0).all(), f\"negative label values {lb[lb < 0]}\"\n",
    "                _, i = np.unique(lb, axis=0, return_index=True)\n",
    "                if len(i) < nl:  # duplicate row check\n",
    "                    lb = lb[i]  # remove duplicates\n",
    "                    if segments:\n",
    "                        segments = [segments[x] for x in i]\n",
    "                    msg = f\"{prefix}WARNING ⚠️ {im_file}: {nl - len(i)} duplicate labels removed\"\n",
    "            else:\n",
    "                ne = 1  # label empty\n",
    "                lb = (\n",
    "                    np.zeros((0, 39), dtype=np.float32)\n",
    "                    if keypoint\n",
    "                    else np.zeros((0, 5), dtype=np.float32)\n",
    "                )\n",
    "        else:\n",
    "            nm = 1  # label missing\n",
    "            lb = (\n",
    "                np.zeros((0, 39), dtype=np.float32)\n",
    "                if keypoint\n",
    "                else np.zeros((0, 5), dtype=np.float32)\n",
    "            )\n",
    "        if keypoint:\n",
    "            keypoints = lb[:, 5:].reshape(-1, 17, 2)\n",
    "        lb = lb[:, :5]\n",
    "        return im_file, lb, shape, segments, keypoints, nm, nf, ne, nc, msg\n",
    "    except Exception as e:\n",
    "        nc = 1\n",
    "        msg = f\"{prefix}WARNING ⚠️ {im_file}: ignoring corrupt image/label: {e}\"\n",
    "        return [None, None, None, None, None, nm, nf, ne, nc, msg]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLODataset(BaseDataset):\n",
    "    cache_version = \"1.0.1\"  # dataset labels *.cache version, >= 1.0.0 for YOLOv8\n",
    "    rand_interp_methods = [\n",
    "        cv2.INTER_NEAREST,\n",
    "        cv2.INTER_LINEAR,\n",
    "        cv2.INTER_CUBIC,\n",
    "        cv2.INTER_AREA,\n",
    "        cv2.INTER_LANCZOS4,\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_path,\n",
    "        imgsz=640,\n",
    "        cache=False,\n",
    "        augment=True,\n",
    "        hyp=None,\n",
    "        prefix=\"\",\n",
    "        rect=False,\n",
    "        batch_size=None,\n",
    "        stride=32,\n",
    "        pad=0.0,\n",
    "        single_cls=False,\n",
    "        names=None,\n",
    "    ):\n",
    "        self.names = names\n",
    "        self.use_segments = None\n",
    "        self.use_keypoints = None\n",
    "\n",
    "        super().__init__(\n",
    "            img_path,\n",
    "            imgsz,\n",
    "            cache,\n",
    "            augment,\n",
    "            hyp,\n",
    "            prefix,\n",
    "            rect,\n",
    "            batch_size,\n",
    "            stride,\n",
    "            pad,\n",
    "            single_cls,\n",
    "        )\n",
    "\n",
    "    def cache_labels(self, path=Path(\"./labels.cache\")):\n",
    "        \"\"\"Cache dataset labels, check images and read shapes.\n",
    "        Args:\n",
    "            path (Path): path where to save the cache file (default: Path('./labels.cache')).\n",
    "        Returns:\n",
    "            (dict): labels.\n",
    "        \"\"\"\n",
    "        x = {\"labels\": []}\n",
    "        nm, nf, ne, nc, msgs = (\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            [],\n",
    "        )  # number missing, found, empty, corrupt, messages\n",
    "        desc = f\"{self.prefix}Scanning {path.parent / path.stem}...\"\n",
    "        total = len(self.im_files)\n",
    "        with ThreadPool(NUM_THREADS) as pool:\n",
    "            results = pool.imap(\n",
    "                func=verify_image_label,\n",
    "                iterable=zip(\n",
    "                    self.im_files,\n",
    "                    self.label_files,\n",
    "                    repeat(self.prefix),\n",
    "                    repeat(self.use_keypoints),\n",
    "                    repeat(len(self.names)),\n",
    "                ),\n",
    "            )\n",
    "            pbar = tqdm(results, desc=desc, total=total, bar_format=TQDM_BAR_FORMAT)\n",
    "            for (\n",
    "                im_file,\n",
    "                lb,\n",
    "                shape,\n",
    "                segments,\n",
    "                keypoint,\n",
    "                nm_f,\n",
    "                nf_f,\n",
    "                ne_f,\n",
    "                nc_f,\n",
    "                msg,\n",
    "            ) in pbar:\n",
    "                nm += nm_f\n",
    "                nf += nf_f\n",
    "                ne += ne_f\n",
    "                nc += nc_f\n",
    "                if im_file:\n",
    "                    x[\"labels\"].append(\n",
    "                        dict(\n",
    "                            im_file=im_file,\n",
    "                            shape=shape,\n",
    "                            cls=lb[:, 0:1],  # n, 1\n",
    "                            bboxes=lb[:, 1:],  # n, 4\n",
    "                            segments=segments,\n",
    "                            keypoints=keypoint,\n",
    "                            normalized=True,\n",
    "                            bbox_format=\"xywh\",\n",
    "                        )\n",
    "                    )\n",
    "                if msg:\n",
    "                    msgs.append(msg)\n",
    "                pbar.desc = f\"{desc} {nf} images, {nm + ne} backgrounds, {nc} corrupt\"\n",
    "            pbar.close()\n",
    "\n",
    "        if msgs:\n",
    "            LOGGER.info(\"\\n\".join(msgs))\n",
    "        if nf == 0:\n",
    "            LOGGER.warning(f\"{self.prefix}WARNING ⚠️ No labels found in {path}.\")\n",
    "        x[\"hash\"] = get_hash(self.label_files + self.im_files)\n",
    "        x[\"results\"] = nf, nm, ne, nc, len(self.im_files)\n",
    "        x[\"msgs\"] = msgs  # warnings\n",
    "        x[\"version\"] = self.cache_version  # cache version\n",
    "        if is_dir_writeable(path.parent):\n",
    "            if path.exists():\n",
    "                path.unlink()  # remove *.cache file if exists\n",
    "            np.save(str(path), x)  # save cache for next time\n",
    "            path.with_suffix(\".cache.npy\").rename(path)  # remove .npy suffix\n",
    "            LOGGER.info(f\"{self.prefix}New cache created: {path}\")\n",
    "        else:\n",
    "            LOGGER.warning(\n",
    "                f\"{self.prefix}WARNING ⚠️ Cache directory {path.parent} is not writeable, cache not saved.\"\n",
    "            )\n",
    "        return x\n",
    "\n",
    "    def get_labels(self):\n",
    "        self.label_files = img2label_paths(self.im_files)\n",
    "        cache_path = Path(self.label_files[0]).parent.with_suffix(\".cache\")\n",
    "        try:\n",
    "            cache, exists = (\n",
    "                np.load(str(cache_path), allow_pickle=True).item(),\n",
    "                True,\n",
    "            )  # load dict\n",
    "            assert cache[\"version\"] == self.cache_version  # matches current version\n",
    "            assert cache[\"hash\"] == get_hash(\n",
    "                self.label_files + self.im_files\n",
    "            )  # identical hash\n",
    "        except (FileNotFoundError, AssertionError, AttributeError):\n",
    "            cache, exists = self.cache_labels(cache_path), False  # run cache ops\n",
    "\n",
    "        # Display cache\n",
    "        nf, nm, ne, nc, n = cache.pop(\n",
    "            \"results\"\n",
    "        )  # found, missing, empty, corrupt, total\n",
    "        if exists and LOCAL_RANK in {-1, 0}:\n",
    "            d = f\"Scanning {cache_path}... {nf} images, {nm + ne} backgrounds, {nc} corrupt\"\n",
    "            tqdm(\n",
    "                None,\n",
    "                desc=self.prefix + d,\n",
    "                total=n,\n",
    "                initial=n,\n",
    "                bar_format=TQDM_BAR_FORMAT,\n",
    "            )  # display cache results\n",
    "            if cache[\"msgs\"]:\n",
    "                LOGGER.info(\"\\n\".join(cache[\"msgs\"]))  # display warnings\n",
    "        if nf == 0:  # number of labels found\n",
    "            raise FileNotFoundError(\n",
    "                f\"{self.prefix}No labels found in {cache_path}, can not start training.\"\n",
    "            )\n",
    "\n",
    "        # Read cache\n",
    "        [cache.pop(k) for k in (\"hash\", \"version\", \"msgs\")]  # remove items\n",
    "        labels = cache[\"labels\"]\n",
    "        self.im_files = [lb[\"im_file\"] for lb in labels]  # update im_files\n",
    "\n",
    "        # Check if the dataset is all boxes or all segments\n",
    "        lengths = (\n",
    "            (len(lb[\"cls\"]), len(lb[\"bboxes\"]), len(lb[\"segments\"])) for lb in labels\n",
    "        )\n",
    "        len_cls, len_boxes, len_segments = (sum(x) for x in zip(*lengths))\n",
    "        if len_segments and len_boxes != len_segments:\n",
    "            LOGGER.warning(\n",
    "                f\"WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = {len_segments}, \"\n",
    "                f\"len(boxes) = {len_boxes}. To resolve this only boxes will be used and all segments will be removed. \"\n",
    "                \"To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\"\n",
    "            )\n",
    "            for lb in labels:\n",
    "                lb[\"segments\"] = []\n",
    "        if len_cls == 0:\n",
    "            raise ValueError(\n",
    "                f\"All labels empty in {cache_path}, can not start training without labels.\"\n",
    "            )\n",
    "        return labels\n",
    "\n",
    "    # TODO: use hyp config to set all these augmentations\n",
    "    # def build_transforms(self, hyp=None):\n",
    "    #     if self.augment:\n",
    "    #         hyp.mosaic = hyp.mosaic if self.augment and not self.rect else 0.0\n",
    "    #         hyp.mixup = hyp.mixup if self.augment and not self.rect else 0.0\n",
    "    #         transforms = v8_transforms(self, self.imgsz, hyp)\n",
    "    #     else:\n",
    "    #         transforms = Compose(\n",
    "    #             [LetterBox(new_shape=(self.imgsz, self.imgsz), scaleup=False)]\n",
    "    #         )\n",
    "    #     transforms.append(\n",
    "    #         Format(\n",
    "    #             bbox_format=\"xywh\",\n",
    "    #             normalize=True,\n",
    "    #             return_mask=self.use_segments,\n",
    "    #             return_keypoint=self.use_keypoints,\n",
    "    #             batch_idx=True,\n",
    "    #             mask_ratio=hyp.mask_ratio,\n",
    "    #             mask_overlap=hyp.overlap_mask,\n",
    "    #         )\n",
    "    #     )\n",
    "    #     return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning coco128/labels/train2017.cache... 128 images, 0 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "def colorstr(*input):\n",
    "    # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world')\n",
    "    *args, string = (\n",
    "        input if len(input) > 1 else (\"blue\", \"bold\", input[0])\n",
    "    )  # color arguments, string\n",
    "    colors = {\n",
    "        \"black\": \"\\033[30m\",  # basic colors\n",
    "        \"red\": \"\\033[31m\",\n",
    "        \"green\": \"\\033[32m\",\n",
    "        \"yellow\": \"\\033[33m\",\n",
    "        \"blue\": \"\\033[34m\",\n",
    "        \"magenta\": \"\\033[35m\",\n",
    "        \"cyan\": \"\\033[36m\",\n",
    "        \"white\": \"\\033[37m\",\n",
    "        \"bright_black\": \"\\033[90m\",  # bright colors\n",
    "        \"bright_red\": \"\\033[91m\",\n",
    "        \"bright_green\": \"\\033[92m\",\n",
    "        \"bright_yellow\": \"\\033[93m\",\n",
    "        \"bright_blue\": \"\\033[94m\",\n",
    "        \"bright_magenta\": \"\\033[95m\",\n",
    "        \"bright_cyan\": \"\\033[96m\",\n",
    "        \"bright_white\": \"\\033[97m\",\n",
    "        \"end\": \"\\033[0m\",  # misc\n",
    "        \"bold\": \"\\033[1m\",\n",
    "        \"underline\": \"\\033[4m\",\n",
    "    }\n",
    "    return \"\".join(colors[x] for x in args) + f\"{string}\" + colors[\"end\"]\n",
    "\n",
    "\n",
    "mode = \"train\"\n",
    "img_path = \"coco128/\"\n",
    "batch = 2\n",
    "imgsz = 224\n",
    "names = (\n",
    "    {\n",
    "        0: \"person\",\n",
    "        1: \"bicycle\",\n",
    "        2: \"car\",\n",
    "        3: \"motorcycle\",\n",
    "        4: \"airplane\",\n",
    "        5: \"bus\",\n",
    "        6: \"train\",\n",
    "        7: \"truck\",\n",
    "        8: \"boat\",\n",
    "        9: \"traffic light\",\n",
    "        10: \"fire hydrant\",\n",
    "        11: \"stop sign\",\n",
    "        12: \"parking meter\",\n",
    "        13: \"bench\",\n",
    "        14: \"bird\",\n",
    "        15: \"cat\",\n",
    "        16: \"dog\",\n",
    "        17: \"horse\",\n",
    "        18: \"sheep\",\n",
    "        19: \"cow\",\n",
    "        20: \"elephant\",\n",
    "        21: \"bear\",\n",
    "        22: \"zebra\",\n",
    "        23: \"giraffe\",\n",
    "        24: \"backpack\",\n",
    "        25: \"umbrella\",\n",
    "        26: \"handbag\",\n",
    "        27: \"tie\",\n",
    "        28: \"suitcase\",\n",
    "        29: \"frisbee\",\n",
    "        30: \"skis\",\n",
    "        31: \"snowboard\",\n",
    "        32: \"sports ball\",\n",
    "        33: \"kite\",\n",
    "        34: \"baseball bat\",\n",
    "        35: \"baseball glove\",\n",
    "        36: \"skateboard\",\n",
    "        37: \"surfboard\",\n",
    "        38: \"tennis racket\",\n",
    "        39: \"bottle\",\n",
    "        40: \"wine glass\",\n",
    "        41: \"cup\",\n",
    "        42: \"fork\",\n",
    "        43: \"knife\",\n",
    "        44: \"spoon\",\n",
    "        45: \"bowl\",\n",
    "        46: \"banana\",\n",
    "        47: \"apple\",\n",
    "        48: \"sandwich\",\n",
    "        49: \"orange\",\n",
    "        50: \"broccoli\",\n",
    "        51: \"carrot\",\n",
    "        52: \"hot dog\",\n",
    "        53: \"pizza\",\n",
    "        54: \"donut\",\n",
    "        55: \"cake\",\n",
    "        56: \"chair\",\n",
    "        57: \"couch\",\n",
    "        58: \"potted plant\",\n",
    "        59: \"bed\",\n",
    "        60: \"dining table\",\n",
    "        61: \"toilet\",\n",
    "        62: \"tv\",\n",
    "        63: \"laptop\",\n",
    "        64: \"mouse\",\n",
    "        65: \"remote\",\n",
    "        66: \"keyboard\",\n",
    "        67: \"cell phone\",\n",
    "        68: \"microwave\",\n",
    "        69: \"oven\",\n",
    "        70: \"toaster\",\n",
    "        71: \"sink\",\n",
    "        72: \"refrigerator\",\n",
    "        73: \"book\",\n",
    "        74: \"clock\",\n",
    "        75: \"vase\",\n",
    "        76: \"scissors\",\n",
    "        77: \"teddy bear\",\n",
    "        78: \"hair drier\",\n",
    "        79: \"toothbrush\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "dataset = YOLODataset(\n",
    "    img_path=img_path,\n",
    "    imgsz=imgsz,\n",
    "    batch_size=batch,\n",
    "    augment=mode == \"train\",  # augmentation\n",
    "    hyp=None,  # TODO: probably add a get_hyps_from_cfg function\n",
    "    rect=False,  # rectangular batches\n",
    "    cache=False,\n",
    "    single_cls=False,\n",
    "    stride=1,\n",
    "    pad=0.0 if mode == \"train\" else 0.5,\n",
    "    prefix=colorstr(f\"{mode}: \"),\n",
    "    names=names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = min(batch, len(dataset))\n",
    "loader = DataLoader\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(6148914691236517205 + RANK)\n",
    "train_loader = loader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch,\n",
    "    shuffle=True,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    # collate_fn=getattr(dataset, \"collate_fn\", None),\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=generator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'im_file': 'coco128/images/train2017/000000000009.jpg',\n",
       " 'cls': array([[45.],\n",
       "        [45.],\n",
       "        [50.],\n",
       "        [45.],\n",
       "        [49.],\n",
       "        [49.],\n",
       "        [49.],\n",
       "        [49.]], dtype=float32),\n",
       " 'bboxes': array([[0.479492 , 0.688771 , 0.955609 , 0.5955   ],\n",
       "        [0.736516 , 0.247188 , 0.498875 , 0.476417 ],\n",
       "        [0.637063 , 0.732938 , 0.494125 , 0.510583 ],\n",
       "        [0.339438 , 0.418896 , 0.678875 , 0.7815   ],\n",
       "        [0.646836 , 0.132552 , 0.118047 , 0.0969375],\n",
       "        [0.773148 , 0.129802 , 0.0907344, 0.0972292],\n",
       "        [0.668297 , 0.226906 , 0.131281 , 0.146896 ],\n",
       "        [0.642859 , 0.0792187, 0.148063 , 0.148062 ]], dtype=float32),\n",
       " 'segments': [],\n",
       " 'keypoints': None,\n",
       " 'normalized': True,\n",
       " 'bbox_format': 'xywh',\n",
       " 'img': array([[[115,  21,   2],\n",
       "         [115,  22,   1],\n",
       "         [115,  22,   1],\n",
       "         ...,\n",
       "         [205, 177, 146],\n",
       "         [198, 174, 140],\n",
       "         [191, 170, 134]],\n",
       " \n",
       "        [[115,  22,   1],\n",
       "         [116,  23,   2],\n",
       "         [117,  24,   1],\n",
       "         ...,\n",
       "         [212, 178, 148],\n",
       "         [210, 175, 142],\n",
       "         [207, 173, 137]],\n",
       " \n",
       "        [[115,  22,   1],\n",
       "         [117,  24,   1],\n",
       "         [119,  27,   2],\n",
       "         ...,\n",
       "         [211, 181, 147],\n",
       "         [211, 180, 143],\n",
       "         [213, 177, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  6,   4,   8],\n",
       "         [ 12,  14,  23],\n",
       "         [  8,  14,  27],\n",
       "         ...,\n",
       "         [122,  77,  59],\n",
       "         [112,  76,  47],\n",
       "         [122,  92,  51]],\n",
       " \n",
       "        [[  4,   0,   5],\n",
       "         [  8,   6,  13],\n",
       "         [ 14,  15,  26],\n",
       "         ...,\n",
       "         [140,  99,  81],\n",
       "         [ 85,  51,  30],\n",
       "         [ 62,  32,   8]],\n",
       " \n",
       "        [[  6,   0,   6],\n",
       "         [  8,   4,  10],\n",
       "         [  9,   9,  18],\n",
       "         ...,\n",
       "         [ 54,  26,   5],\n",
       "         [ 31,   5,   3],\n",
       "         [ 23,   0,  10]]], dtype=uint8),\n",
       " 'ori_shape': (480, 640),\n",
       " 'resized_shape': (168, 224),\n",
       " 'ratio_pad': (0.35, 0.35)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'im_file': 'coco128/images/train2017/000000000009.jpg',\n",
       " 'cls': array([[45.],\n",
       "        [45.],\n",
       "        [50.],\n",
       "        [45.],\n",
       "        [49.],\n",
       "        [49.],\n",
       "        [49.],\n",
       "        [49.]], dtype=float32),\n",
       " 'bboxes': array([[0.479492 , 0.688771 , 0.955609 , 0.5955   ],\n",
       "        [0.736516 , 0.247188 , 0.498875 , 0.476417 ],\n",
       "        [0.637063 , 0.732938 , 0.494125 , 0.510583 ],\n",
       "        [0.339438 , 0.418896 , 0.678875 , 0.7815   ],\n",
       "        [0.646836 , 0.132552 , 0.118047 , 0.0969375],\n",
       "        [0.773148 , 0.129802 , 0.0907344, 0.0972292],\n",
       "        [0.668297 , 0.226906 , 0.131281 , 0.146896 ],\n",
       "        [0.642859 , 0.0792187, 0.148063 , 0.148062 ]], dtype=float32),\n",
       " 'segments': [],\n",
       " 'keypoints': None,\n",
       " 'normalized': True,\n",
       " 'bbox_format': 'xywh',\n",
       " 'img': array([[[115,  21,   2],\n",
       "         [115,  22,   1],\n",
       "         [115,  22,   1],\n",
       "         ...,\n",
       "         [205, 177, 146],\n",
       "         [198, 174, 140],\n",
       "         [191, 170, 134]],\n",
       " \n",
       "        [[115,  22,   1],\n",
       "         [116,  23,   2],\n",
       "         [117,  24,   1],\n",
       "         ...,\n",
       "         [212, 178, 148],\n",
       "         [210, 175, 142],\n",
       "         [207, 173, 137]],\n",
       " \n",
       "        [[115,  22,   1],\n",
       "         [117,  24,   1],\n",
       "         [119,  27,   2],\n",
       "         ...,\n",
       "         [211, 181, 147],\n",
       "         [211, 180, 143],\n",
       "         [213, 177, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  6,   4,   8],\n",
       "         [ 12,  14,  23],\n",
       "         [  8,  14,  27],\n",
       "         ...,\n",
       "         [122,  77,  59],\n",
       "         [112,  76,  47],\n",
       "         [122,  92,  51]],\n",
       " \n",
       "        [[  4,   0,   5],\n",
       "         [  8,   6,  13],\n",
       "         [ 14,  15,  26],\n",
       "         ...,\n",
       "         [140,  99,  81],\n",
       "         [ 85,  51,  30],\n",
       "         [ 62,  32,   8]],\n",
       " \n",
       "        [[  6,   0,   6],\n",
       "         [  8,   4,  10],\n",
       "         [  9,   9,  18],\n",
       "         ...,\n",
       "         [ 54,  26,   5],\n",
       "         [ 31,   5,   3],\n",
       "         [ 23,   0,  10]]], dtype=uint8),\n",
       " 'ori_shape': (480, 640),\n",
       " 'resized_shape': (168, 224),\n",
       " 'ratio_pad': (0.35, 0.35)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 224, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"img\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'im_file': 'coco128/images/train2017/000000000025.jpg',\n",
       " 'cls': array([[23.],\n",
       "        [23.]], dtype=float32),\n",
       " 'bboxes': array([[0.770336, 0.489695, 0.335891, 0.697559],\n",
       "        [0.185977, 0.901608, 0.206297, 0.129554]], dtype=float32),\n",
       " 'segments': [],\n",
       " 'keypoints': None,\n",
       " 'normalized': True,\n",
       " 'bbox_format': 'xywh',\n",
       " 'img': array([[[  3,   8,   7],\n",
       "         [  4,   9,   8],\n",
       "         [  5,  10,   9],\n",
       "         ...,\n",
       "         [191, 192, 192],\n",
       "         [158, 160, 161],\n",
       "         [137, 141, 142]],\n",
       " \n",
       "        [[  4,   9,   8],\n",
       "         [  4,   9,   8],\n",
       "         [  4,   9,   8],\n",
       "         ...,\n",
       "         [225, 221, 217],\n",
       "         [214, 211, 209],\n",
       "         [209, 207, 207]],\n",
       " \n",
       "        [[  5,  10,   9],\n",
       "         [  4,   9,   8],\n",
       "         [  3,   8,   7],\n",
       "         ...,\n",
       "         [228, 220, 211],\n",
       "         [229, 222, 215],\n",
       "         [228, 221, 216]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 80, 142, 165],\n",
       "         [ 66, 139, 160],\n",
       "         [ 48, 132, 152],\n",
       "         ...,\n",
       "         [ 67,  92, 133],\n",
       "         [107, 133, 168],\n",
       "         [110, 135, 169]],\n",
       " \n",
       "        [[ 89, 169, 181],\n",
       "         [ 55, 142, 153],\n",
       "         [ 69, 160, 170],\n",
       "         ...,\n",
       "         [ 91, 120, 152],\n",
       "         [ 86, 116, 145],\n",
       "         [ 77, 108, 136]],\n",
       " \n",
       "        [[ 61, 156, 159],\n",
       "         [ 35, 132, 136],\n",
       "         [ 48, 142, 146],\n",
       "         ...,\n",
       "         [ 83, 111, 140],\n",
       "         [ 89, 119, 146],\n",
       "         [ 86, 117, 142]]], dtype=uint8),\n",
       " 'ori_shape': (426, 640),\n",
       " 'resized_shape': (150, 224),\n",
       " 'ratio_pad': (0.352112676056338, 0.35)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
