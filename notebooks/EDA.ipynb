{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 21:56:47,040\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\"data\", download=True, train=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(\"data\", download=True, train=False)\n",
    "\n",
    "train_dataset: ray.data.Dataset = ray.data.from_torch(train_dataset)\n",
    "test_dataset: ray.data.Dataset = ray.data.from_torch(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 21:57:01,299\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(convert_batch_to_numpy)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset]: Run `pip install tqdm` to enable progress reporting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 21:57:02,029\tWARNING plan.py:523 -- Warning: The Ray cluster currently does not have any available CPUs. The Dataset job will hang unless more CPUs are freed up. A common reason is that cluster resources are used by Actors or Tune trials; see the following link for more details: https://docs.ray.io/en/master/data/dataset-internals.html#datasets-and-tune\n",
      "2023-03-16 21:57:02,030\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(convert_batch_to_numpy)]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "import numpy as np\n",
    "from PIL.Image import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "def convert_batch_to_numpy(batch: Tuple[Image, int]) -> Dict[str, np.ndarray]:\n",
    "    images = np.stack([np.array(image) for image, _ in batch])\n",
    "    labels = np.array([label for _, label in batch])\n",
    "    return {\"image\": images, \"label\": labels}\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map_batches(convert_batch_to_numpy).fully_executed()\n",
    "test_dataset = test_dataset.map_batches(convert_batch_to_numpy).fully_executed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.take(1)[0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 20:39:59,622\tINFO instantiator.py:21 -- Created a temporary directory at /var/folders/p4/kcmtkxw53z54k341vwwykts80000gn/T/tmpyqs1s8xb\n",
      "2023-03-16 20:39:59,623\tINFO instantiator.py:76 -- Writing /var/folders/p4/kcmtkxw53z54k341vwwykts80000gn/T/tmpyqs1s8xb/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from ray import train\n",
    "from ray.air import session, Checkpoint\n",
    "from ray.train.torch import TorchCheckpoint\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    model = train.torch.prepare_model(Net())\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    train_dataset_shard = session.get_dataset_shard(\"train\")\n",
    "\n",
    "    for epoch in range(2):\n",
    "        running_loss = 0.0\n",
    "        train_dataset_batches = train_dataset_shard.iter_torch_batches(\n",
    "            batch_size=config[\"batch_size\"], device=train.torch.get_device()\n",
    "        )\n",
    "        for i, batch in enumerate(train_dataset_batches):\n",
    "            # get the inputs and labels\n",
    "            inputs, labels = batch[\"image\"], batch[\"label\"]\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        metrics = dict(running_loss=running_loss)\n",
    "        checkpoint = TorchCheckpoint.from_state_dict(model.state_dict())\n",
    "        session.report(metrics, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import TorchVisionPreprocessor\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "preprocessor = TorchVisionPreprocessor(columns=[\"image\"], transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-03-16 20:40:30</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:02.49        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.0/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 3.0/10 CPUs, 0/0 GPUs, 0.0/5.1 GiB heap, 0.0/2.0 GiB objects\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_bac1f_00000</td><td>RUNNING </td><td>127.0.0.1:93323</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=93323)\u001b[0m 2023-03-16 20:40:30,111\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[TorchVisionPreprocessor] -> AllToAllOperator[randomize_block_order]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m 2023-03-16 20:40:35,759\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=93323)\u001b[0m /opt/homebrew/Caskroom/miniforge/base/envs/xnn/lib/python3.10/site-packages/ray/data/_internal/bulk_dataset_iterator.py:108: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=93323)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m 2023-03-16 20:40:36,852\tINFO train_loop_utils.py:255 -- Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m 2023-03-16 20:40:36,852\tINFO train_loop_utils.py:315 -- Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93339)\u001b[0m [1,  2000] loss: 2.192\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m [1,  2000] loss: 2.198\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93339)\u001b[0m [1,  4000] loss: 1.846\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m [1,  4000] loss: 1.866\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93339)\u001b[0m [1,  6000] loss: 1.666\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m [1,  6000] loss: 1.668\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93339)\u001b[0m [1,  8000] loss: 1.594\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m [1,  8000] loss: 1.574\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93339)\u001b[0m [1, 10000] loss: 1.506\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m [1, 10000] loss: 1.502\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93339)\u001b[0m [1, 12000] loss: 1.464\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m [1, 12000] loss: 1.479\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  _time_this_iter_s</th><th style=\"text-align: right;\">  _timestamp</th><th style=\"text-align: right;\">  _training_iteration</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  running_loss</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_bac1f_00000</td><td style=\"text-align: right;\">            26.2573</td><td style=\"text-align: right;\">  1678970481</td><td style=\"text-align: right;\">                    2</td><td>2023-03-16_20-41-21</td><td>False </td><td>                </td><td>c67991d8ca8546fa9c0f3b61efab92b2</td><td>mac       </td><td style=\"text-align: right;\">                         2</td><td>127.0.0.1</td><td style=\"text-align: right;\">93323</td><td style=\"text-align: right;\">       610.467</td><td>True               </td><td style=\"text-align: right;\">             51.3855</td><td style=\"text-align: right;\">           26.2526</td><td style=\"text-align: right;\">       51.3855</td><td style=\"text-align: right;\"> 1678970481</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>bac1f_00000</td><td style=\"text-align: right;\">     0.156917</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93339)\u001b[0m [2,  2000] loss: 1.442\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m [2,  2000] loss: 1.423\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93339)\u001b[0m [2,  4000] loss: 1.363\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m [2,  4000] loss: 1.411\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93339)\u001b[0m [2,  6000] loss: 1.348\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m [2,  6000] loss: 1.348\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93339)\u001b[0m [2,  8000] loss: 1.347\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m [2,  8000] loss: 1.322\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93339)\u001b[0m [2, 10000] loss: 1.283\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m [2, 10000] loss: 1.288\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93339)\u001b[0m [2, 12000] loss: 1.247\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=93338)\u001b[0m [2, 12000] loss: 1.286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 20:41:23,972\tINFO tune.py:798 -- Total run time: 56.38 seconds (56.35 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "use_gpu = ray.available_resources().get(\"GPU\", 0) >= 2\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config={\"batch_size\": 2},\n",
    "    datasets={\"train\": train_dataset},\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=use_gpu),\n",
    "    preprocessor=preprocessor\n",
    ")\n",
    "result = trainer.fit()\n",
    "latest_checkpoint = result.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 20:42:26,990\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[TorchVisionPreprocessor]\n",
      "2023-03-16 20:42:30,416\tWARNING plan.py:523 -- Warning: The Ray cluster currently does not have any available CPUs. The Dataset job will hang unless more CPUs are freed up. A common reason is that cluster resources are used by Actors or Tune trials; see the following link for more details: https://docs.ray.io/en/master/data/dataset-internals.html#datasets-and-tune\n",
      "2023-03-16 20:42:30,419\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(ScoringWrapper)]\n"
     ]
    }
   ],
   "source": [
    "from ray.train.torch import TorchPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "\n",
    "batch_predictor = BatchPredictor.from_checkpoint(\n",
    "    checkpoint=latest_checkpoint,\n",
    "    predictor_cls=TorchPredictor,\n",
    "    model=Net(),\n",
    ")\n",
    "\n",
    "outputs: ray.data.Dataset = batch_predictor.predict(\n",
    "    data=test_dataset,\n",
    "    dtype=torch.float,\n",
    "    feature_columns=[\"image\"],\n",
    "    keep_columns=[\"label\"],\n",
    "    # We will use GPU if available.\n",
    "    num_gpus_per_worker=ray.available_resources().get(\"GPU\", 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 20:42:32,709\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(convert_logits_to_classes)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 5, 'label': 3}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def convert_logits_to_classes(df):\n",
    "    best_class = df[\"predictions\"].map(lambda x: x.argmax())\n",
    "    df[\"prediction\"] = best_class\n",
    "    return df[[\"prediction\", \"label\"]]\n",
    "\n",
    "\n",
    "predictions = outputs.map_batches(convert_logits_to_classes)\n",
    "\n",
    "predictions.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 20:42:32,750\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(calculate_prediction_scores)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 5, 'label': 3, 'correct': False}\n"
     ]
    }
   ],
   "source": [
    "def calculate_prediction_scores(df):\n",
    "    df[\"correct\"] = df[\"prediction\"] == df[\"label\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "scores = predictions.map_batches(calculate_prediction_scores)\n",
    "\n",
    "scores.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 20:42:32,779\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[aggregate]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5584"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.sum(on=\"correct\") / scores.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=93408)\u001b[0m INFO 2023-03-16 20:42:33,569 controller 93408 http_state.py:129 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-6fd0598e2fa7ee7d98de2442fa8b11b9e6f389adce467a9cb617f642' on node '6fd0598e2fa7ee7d98de2442fa8b11b9e6f389adce467a9cb617f642' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=93409)\u001b[0m INFO:     Started server process [93409]\n",
      "2023-03-16 20:42:34,252\tINFO api.py:254 -- Started detached Serve instance in namespace \"serve\".\n",
      "2023-03-16 20:42:34,266\tINFO client.py:540 -- Updating deployment 'PredictorDeployment'. component=serve deployment=PredictorDeployment\n",
      "\u001b[2m\u001b[36m(ServeController pid=93408)\u001b[0m INFO 2023-03-16 20:42:34,304 controller 93408 deployment_state.py:1333 - Adding 1 replica to deployment 'PredictorDeployment'.\n",
      "2023-03-16 20:42:36,278\tINFO client.py:555 -- Deployment 'PredictorDeployment' is ready at `http://127.0.0.1:8000/`. component=serve deployment=PredictorDeployment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayServeSyncHandle(deployment='PredictorDeployment')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import serve\n",
    "from ray.serve import PredictorDeployment\n",
    "from ray.serve.http_adapters import json_to_ndarray\n",
    "\n",
    "\n",
    "serve.run(\n",
    "    PredictorDeployment.bind(\n",
    "        TorchPredictor,\n",
    "        latest_checkpoint,\n",
    "        model=Net(),\n",
    "        http_adapter=json_to_ndarray,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_dataset.take(1)[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [818.4107055664062,\n",
       "  -54.2667121887207,\n",
       "  205.0116424560547,\n",
       "  244.10177612304688,\n",
       "  -44.22550964355469,\n",
       "  278.5929870605469,\n",
       "  -436.99383544921875,\n",
       "  -129.3734588623047,\n",
       "  187.9035186767578,\n",
       "  -387.4586486816406]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=93409)\u001b[0m INFO 2023-03-16 20:42:36,385 http_proxy 127.0.0.1 http_proxy.py:373 - POST / 200 14.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:PredictorDeployment pid=93410)\u001b[0m INFO 2023-03-16 20:42:36,384 PredictorDeployment PredictorDeployment#FoDFLK replica.py:518 - HANDLE __call__ OK 11.2ms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "payload = {\"array\": image.tolist(), \"dtype\": \"float32\"}\n",
    "response = requests.post(\"http://localhost:8000/\", json=payload)\n",
    "response.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
